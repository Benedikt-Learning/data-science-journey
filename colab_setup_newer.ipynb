{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_WpWke0hxXN"
      },
      "source": [
        "# UCF-Crime Anomaly Detection - Colab Setup\n",
        "\n",
        "Setup notebook for running MIL Ranking Loss re-implementation on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn-H35qBhxXP"
      },
      "source": [
        "## 1. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN-HxfyThxXQ",
        "outputId": "7c482a08-c147-4d8a-be98-1203078086bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 30 11:31:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              8W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "PyTorch CUDA available: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# Verify PyTorch can use GPU\n",
        "import torch\n",
        "print(f\"\\nPyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WNVNwnhxXS"
      },
      "source": [
        "## 2. Mount Google Drive\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Upload features.zip and annotations.zip to Google Drive\n",
        "2. Google Drive structure:\n",
        "```\n",
        "MyDrive/\n",
        "└── Colab Notebooks/\n",
        "    └── data_distribution/\n",
        "        ├── features.zip\n",
        "        └── annotations.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDQpbfvdhxXT",
        "outputId": "d8c6867f-4d2b-452f-e1af-19f2463fa573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8LF-NwXhxXU"
      },
      "source": [
        "## 3. Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjIMc96hxXU"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/KwonPodo/MILRankingLoss_Sultani2018_ReImplementation.git\n",
        "%cd MILRankingLoss_Sultani2018_ReImplementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzjvGTh1hxXV"
      },
      "source": [
        "## 4. Install Packages\n",
        "\n",
        "**Note:** Using `requirements-colab.txt` to avoid package conflicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrGcuM45hxXV",
        "outputId": "e26f213e-1042-4916-f919-fab2aeb46bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "NumPy: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install minimal packages for Colab (avoid conflicts)\n",
        "!pip install -r requirements-colab.txt -q\n",
        "\n",
        "# Check installed package versions\n",
        "import torch\n",
        "import numpy as np\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrp7JgohxXV"
      },
      "source": [
        "## 5. Extract Data\n",
        "\n",
        "Extract features from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og3JPsgnhxXW",
        "outputId": "040d5c74-f3d3-461b-c837-cbdc5e6d4ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "replace data/features/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Features extracted\n",
            "total 4.0K\n",
            "drwxr-xr-x 17 root root 4.0K Nov 30 11:31 features\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Verify working directory\n",
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!pwd\n",
        "\n",
        "# Create data directory\n",
        "!mkdir -p data\n",
        "\n",
        "# Google Drive path\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data_distribution'\n",
        "\n",
        "# Extract features\n",
        "!cp \"{DRIVE_DATA_PATH}/features.zip\" data/\n",
        "!unzip -q data/features.zip -d data/\n",
        "!rm data/features.zip\n",
        "\n",
        "print(\"Features extracted\")\n",
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34-OkYIShxXW"
      },
      "source": [
        "## 6. Extract Annotations\n",
        "\n",
        "Extract annotation files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdKZqOuphxXX",
        "outputId": "bb5ae48c-828d-4ee9-dade-88cb0539a230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations extracted\n",
            "total 100K\n",
            "-rwxr-xr-x 1 root root 16K Jan  3  2023 Temporal_Anomaly_Annotation_for_Testing_Videos.txt\n",
            "-rwxr-xr-x 1 root root 13K Oct 31 17:28 test_set.txt\n",
            "-rwxr-xr-x 1 root root 66K Oct 31 17:28 train_set.txt\n"
          ]
        }
      ],
      "source": [
        "# Extract annotations from Google Drive\n",
        "!cp \"{DRIVE_DATA_PATH}/annotations.zip\" data/\n",
        "!unzip -q data/annotations.zip -d data/\n",
        "!rm data/annotations.zip\n",
        "\n",
        "print(\"Annotations extracted\")\n",
        "!ls -lh data/annotations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Y4lj_uhxXX"
      },
      "source": [
        "## 7. Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBZl7KnnhxXX",
        "outputId": "606d35ad-80b3-4cf8-cdac-213cdbf25ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Abuse\tAssault    Fighting\t  Shooting     Testing_Normal_Videos_Anomaly\n",
            "Arrest\tBurglary   RoadAccidents  Shoplifting  Training_Normal_Videos_Anomaly\n",
            "Arson\tExplosion  Robbery\t  Stealing     Vandalism\n",
            "Train samples:\n",
            "1610 data/annotations/train_set.txt\n",
            "Test samples:\n",
            "290 data/annotations/test_set.txt\n"
          ]
        }
      ],
      "source": [
        "# Verify working directory\n",
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Check feature categories\n",
        "!ls data/features/\n",
        "\n",
        "# Check sample counts\n",
        "!echo \"Train samples:\"\n",
        "!wc -l data/annotations/train_set.txt\n",
        "!echo \"Test samples:\"\n",
        "!wc -l data/annotations/test_set.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXD8k0kEhxXY"
      },
      "source": [
        "## 8. Test Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4NiamhghxXY",
        "outputId": "e747739c-467a-4016-f93c-e0d387ec1369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Total samples in dataset: 1610\n",
            "Positive samples: 810\n",
            "Negative samples: 800\n",
            "\n",
            "First batch:\n",
            "\n",
            "Positive bags: torch.Size([30, 32, 4096])\n",
            "\n",
            "Negative bags: torch.Size([30, 32, 4096])\n",
            "Batch 0: pos=30, neg=30\n",
            "Batch 1: pos=30, neg=30\n",
            "Batch 2: pos=30, neg=30\n",
            "Batch 3: pos=30, neg=30\n"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python scripts/test_dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Ho4KP6hxXY"
      },
      "source": [
        "## 9. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg155EkHhxXY",
        "outputId": "fb3becd6-1924-4c6e-ec54-dc9b1c355d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Model architecture:\n",
            "AnomalyDetector(\n",
            "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.6, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Total parameters: 2,114,113\n",
            "\n",
            "Positive scores shape: torch.Size([30, 32])\n",
            "Negative scores shape: torch.Size([30, 32])\n",
            "Score range: [0.4717, 0.5796]\n",
            "\n",
            "Total loss: 1.0406\n",
            "  Ranking loss: 0.9999\n",
            "  Smoothness loss: 0.6181\n",
            "  Sparsity loss: 507.8174\n",
            "\n",
            "Training mode loss: 1.0406\n"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python scripts/test_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydP3YZTFhxXY"
      },
      "source": [
        "## 10. Start Training\n",
        "\n",
        "### Option 1: Train without WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHgnmRshhxXY",
        "outputId": "e955425d-cf5f-4feb-f6df-0803188c47c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Loaded config from configs/default.yaml\n",
            "Using device: cuda\n",
            "Train dataset: 1610 videos\n",
            "Positive samples: 810\n",
            "Negative samples: 800\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Total batches per epoch: 26\n",
            "Model parameters: 2,114,113\n",
            "Optimizer: adam\n",
            "\n",
            "Starting training for 100 epochs...\n",
            "Epoch 1:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100% 26/26 [01:04<00:00,  2.47s/it, loss=1.0375, rank=0.9993]\n",
            "Epoch 1/100\n",
            "  Loss: 1.0376\n",
            "  Ranking: 0.9991\n",
            "  Smoothness: 0.0337\n",
            "  Sparsity: 480.9726\n",
            "  Saved checkpoint: checkpoints/epoch_1.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 2: 100% 26/26 [01:02<00:00,  2.42s/it, loss=1.0351, rank=0.9974]\n",
            "Epoch 2/100\n",
            "  Loss: 1.0365\n",
            "  Ranking: 0.9986\n",
            "  Smoothness: 0.0316\n",
            "  Sparsity: 474.5381\n",
            "  Saved checkpoint: checkpoints/epoch_2.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 3: 100% 26/26 [01:01<00:00,  2.35s/it, loss=1.0353, rank=0.9981]\n",
            "Epoch 3/100\n",
            "  Loss: 1.0358\n",
            "  Ranking: 0.9983\n",
            "  Smoothness: 0.0395\n",
            "  Sparsity: 468.5425\n",
            "  Saved checkpoint: checkpoints/epoch_3.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 4: 100% 26/26 [01:02<00:00,  2.41s/it, loss=1.0327, rank=0.9956]\n",
            "Epoch 4/100\n",
            "  Loss: 1.0336\n",
            "  Ranking: 0.9965\n",
            "  Smoothness: 0.0889\n",
            "  Sparsity: 463.8808\n",
            "  Saved checkpoint: checkpoints/epoch_4.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 5: 100% 26/26 [01:02<00:00,  2.40s/it, loss=1.0183, rank=0.9813]\n",
            "Epoch 5/100\n",
            "  Loss: 1.0272\n",
            "  Ranking: 0.9901\n",
            "  Smoothness: 0.3165\n",
            "  Sparsity: 462.7753\n",
            "  Saved checkpoint: checkpoints/epoch_5.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 6: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.9944, rank=0.9578]\n",
            "Epoch 6/100\n",
            "  Loss: 1.0140\n",
            "  Ranking: 0.9767\n",
            "  Smoothness: 1.5048\n",
            "  Sparsity: 464.6150\n",
            "  Saved checkpoint: checkpoints/epoch_6.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 7: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.9626, rank=0.9260]\n",
            "Epoch 7/100\n",
            "  Loss: 0.9911\n",
            "  Ranking: 0.9539\n",
            "  Smoothness: 5.2399\n",
            "  Sparsity: 459.5910\n",
            "  Saved checkpoint: checkpoints/epoch_7.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 8: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.9144, rank=0.8775]\n",
            "Epoch 8/100\n",
            "  Loss: 0.9651\n",
            "  Ranking: 0.9281\n",
            "  Smoothness: 10.1931\n",
            "  Sparsity: 451.7853\n",
            "  Saved checkpoint: checkpoints/epoch_8.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 9: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.9314, rank=0.8963]\n",
            "Epoch 9/100\n",
            "  Loss: 0.9322\n",
            "  Ranking: 0.8956\n",
            "  Smoothness: 18.1626\n",
            "  Sparsity: 438.9566\n",
            "  Saved checkpoint: checkpoints/epoch_9.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 10: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8542, rank=0.8198]\n",
            "Epoch 10/100\n",
            "  Loss: 0.9034\n",
            "  Ranking: 0.8680\n",
            "  Smoothness: 29.7606\n",
            "  Sparsity: 413.1727\n",
            "  Saved checkpoint: checkpoints/epoch_10.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 11: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.8499, rank=0.8179]\n",
            "Epoch 11/100\n",
            "  Loss: 0.8781\n",
            "  Ranking: 0.8436\n",
            "  Smoothness: 37.8653\n",
            "  Sparsity: 392.4199\n",
            "  Saved checkpoint: checkpoints/epoch_11.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 12: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8880, rank=0.8588]\n",
            "Epoch 12/100\n",
            "  Loss: 0.8498\n",
            "  Ranking: 0.8182\n",
            "  Smoothness: 50.7010\n",
            "  Sparsity: 343.5123\n",
            "  Saved checkpoint: checkpoints/epoch_12.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 13: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8435, rank=0.8212]\n",
            "Epoch 13/100\n",
            "  Loss: 0.8257\n",
            "  Ranking: 0.7963\n",
            "  Smoothness: 60.3190\n",
            "  Sparsity: 307.6057\n",
            "  Saved checkpoint: checkpoints/epoch_13.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 14: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6930, rank=0.6697]\n",
            "Epoch 14/100\n",
            "  Loss: 0.7937\n",
            "  Ranking: 0.7665\n",
            "  Smoothness: 80.0147\n",
            "  Sparsity: 260.3102\n",
            "  Saved checkpoint: checkpoints/epoch_14.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 15: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.7425, rank=0.7229]\n",
            "Epoch 15/100\n",
            "  Loss: 0.7615\n",
            "  Ranking: 0.7385\n",
            "  Smoothness: 85.2982\n",
            "  Sparsity: 201.5434\n",
            "  Saved checkpoint: checkpoints/epoch_15.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 16: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.6957, rank=0.6751]\n",
            "Epoch 16/100\n",
            "  Loss: 0.7387\n",
            "  Ranking: 0.7181\n",
            "  Smoothness: 91.4944\n",
            "  Sparsity: 166.9637\n",
            "  Saved checkpoint: checkpoints/epoch_16.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 17: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.7162, rank=0.7007]\n",
            "Epoch 17/100\n",
            "  Loss: 0.7189\n",
            "  Ranking: 0.6989\n",
            "  Smoothness: 92.0162\n",
            "  Sparsity: 157.4646\n",
            "  Saved checkpoint: checkpoints/epoch_17.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 18: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6096, rank=0.5867]\n",
            "Epoch 18/100\n",
            "  Loss: 0.7157\n",
            "  Ranking: 0.6944\n",
            "  Smoothness: 104.8757\n",
            "  Sparsity: 161.5662\n",
            "  Saved checkpoint: checkpoints/epoch_18.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 19: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.7825, rank=0.7514]\n",
            "Epoch 19/100\n",
            "  Loss: 0.7049\n",
            "  Ranking: 0.6841\n",
            "  Smoothness: 104.6911\n",
            "  Sparsity: 155.8092\n",
            "  Saved checkpoint: checkpoints/epoch_19.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 20: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.7277, rank=0.6986]\n",
            "Epoch 20/100\n",
            "  Loss: 0.6931\n",
            "  Ranking: 0.6737\n",
            "  Smoothness: 98.4537\n",
            "  Sparsity: 144.0865\n",
            "  Saved checkpoint: checkpoints/epoch_20.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 21: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5977, rank=0.5737]\n",
            "Epoch 21/100\n",
            "  Loss: 0.6792\n",
            "  Ranking: 0.6583\n",
            "  Smoothness: 107.9645\n",
            "  Sparsity: 152.8774\n",
            "  Saved checkpoint: checkpoints/epoch_21.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 22: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.6999, rank=0.6803]\n",
            "Epoch 22/100\n",
            "  Loss: 0.6560\n",
            "  Ranking: 0.6362\n",
            "  Smoothness: 103.6883\n",
            "  Sparsity: 143.9402\n",
            "  Saved checkpoint: checkpoints/epoch_22.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 23: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.8039, rank=0.7922]\n",
            "Epoch 23/100\n",
            "  Loss: 0.6673\n",
            "  Ranking: 0.6494\n",
            "  Smoothness: 98.1108\n",
            "  Sparsity: 125.0259\n",
            "Epoch 24: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.6014, rank=0.5763]\n",
            "Epoch 24/100\n",
            "  Loss: 0.6570\n",
            "  Ranking: 0.6358\n",
            "  Smoothness: 115.8001\n",
            "  Sparsity: 149.4256\n",
            "Epoch 25: 100% 26/26 [01:03<00:00,  2.43s/it, loss=0.6301, rank=0.6111]\n",
            "Epoch 25/100\n",
            "  Loss: 0.6577\n",
            "  Ranking: 0.6363\n",
            "  Smoothness: 116.0188\n",
            "  Sparsity: 150.5183\n",
            "Epoch 26: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.6264, rank=0.5988]\n",
            "Epoch 26/100\n",
            "  Loss: 0.6268\n",
            "  Ranking: 0.6071\n",
            "  Smoothness: 110.1000\n",
            "  Sparsity: 136.6329\n",
            "  Saved checkpoint: checkpoints/epoch_26.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 27: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.5949, rank=0.5721]\n",
            "Epoch 27/100\n",
            "  Loss: 0.6409\n",
            "  Ranking: 0.6209\n",
            "  Smoothness: 108.1847\n",
            "  Sparsity: 141.5359\n",
            "Epoch 28: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.6831, rank=0.6614]\n",
            "Epoch 28/100\n",
            "  Loss: 0.6267\n",
            "  Ranking: 0.6053\n",
            "  Smoothness: 118.9159\n",
            "  Sparsity: 149.2820\n",
            "  Saved checkpoint: checkpoints/epoch_28.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 29: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.6882, rank=0.6698]\n",
            "Epoch 29/100\n",
            "  Loss: 0.6284\n",
            "  Ranking: 0.6096\n",
            "  Smoothness: 104.3121\n",
            "  Sparsity: 129.7739\n",
            "Epoch 30: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6020, rank=0.5874]\n",
            "Epoch 30/100\n",
            "  Loss: 0.6252\n",
            "  Ranking: 0.6044\n",
            "  Smoothness: 117.8821\n",
            "  Sparsity: 142.5926\n",
            "  Saved checkpoint: checkpoints/epoch_30.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 31: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5943, rank=0.5782]\n",
            "Epoch 31/100\n",
            "  Loss: 0.6176\n",
            "  Ranking: 0.5976\n",
            "  Smoothness: 113.0824\n",
            "  Sparsity: 137.0418\n",
            "  Saved checkpoint: checkpoints/epoch_31.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 32: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.5618, rank=0.5416]\n",
            "Epoch 32/100\n",
            "  Loss: 0.6138\n",
            "  Ranking: 0.5946\n",
            "  Smoothness: 109.4350\n",
            "  Sparsity: 130.9676\n",
            "  Saved checkpoint: checkpoints/epoch_32.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 33: 100% 26/26 [01:04<00:00,  2.46s/it, loss=0.5198, rank=0.5035]\n",
            "Epoch 33/100\n",
            "  Loss: 0.6084\n",
            "  Ranking: 0.5882\n",
            "  Smoothness: 115.1455\n",
            "  Sparsity: 136.5571\n",
            "  Saved checkpoint: checkpoints/epoch_33.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 34: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.6168, rank=0.5961]\n",
            "Epoch 34/100\n",
            "  Loss: 0.6057\n",
            "  Ranking: 0.5857\n",
            "  Smoothness: 114.3990\n",
            "  Sparsity: 134.9616\n",
            "  Saved checkpoint: checkpoints/epoch_34.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 35: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.7752, rank=0.7549]\n",
            "Epoch 35/100\n",
            "  Loss: 0.6028\n",
            "  Ranking: 0.5818\n",
            "  Smoothness: 118.6778\n",
            "  Sparsity: 142.9282\n",
            "  Saved checkpoint: checkpoints/epoch_35.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 36: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6980, rank=0.6772]\n",
            "Epoch 36/100\n",
            "  Loss: 0.5928\n",
            "  Ranking: 0.5707\n",
            "  Smoothness: 125.3819\n",
            "  Sparsity: 150.9625\n",
            "  Saved checkpoint: checkpoints/epoch_36.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 37: 100% 26/26 [01:03<00:00,  2.46s/it, loss=0.6767, rank=0.6568]\n",
            "Epoch 37/100\n",
            "  Loss: 0.5950\n",
            "  Ranking: 0.5717\n",
            "  Smoothness: 132.5752\n",
            "  Sparsity: 159.0052\n",
            "Epoch 38: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.6110, rank=0.5815]\n",
            "Epoch 38/100\n",
            "  Loss: 0.6076\n",
            "  Ranking: 0.5842\n",
            "  Smoothness: 134.4324\n",
            "  Sparsity: 158.4010\n",
            "Epoch 39: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.6224, rank=0.6088]\n",
            "Epoch 39/100\n",
            "  Loss: 0.5902\n",
            "  Ranking: 0.5677\n",
            "  Smoothness: 127.7802\n",
            "  Sparsity: 154.3882\n",
            "  Saved checkpoint: checkpoints/epoch_39.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 40: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.5098, rank=0.4892]\n",
            "Epoch 40/100\n",
            "  Loss: 0.5872\n",
            "  Ranking: 0.5670\n",
            "  Smoothness: 114.8599\n",
            "  Sparsity: 138.6194\n",
            "  Saved checkpoint: checkpoints/epoch_40.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 41: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5230, rank=0.4990]\n",
            "Epoch 41/100\n",
            "  Loss: 0.5917\n",
            "  Ranking: 0.5687\n",
            "  Smoothness: 133.1585\n",
            "  Sparsity: 155.1772\n",
            "Epoch 42: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4875, rank=0.4697]\n",
            "Epoch 42/100\n",
            "  Loss: 0.5705\n",
            "  Ranking: 0.5492\n",
            "  Smoothness: 121.1375\n",
            "  Sparsity: 144.6272\n",
            "  Saved checkpoint: checkpoints/epoch_42.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 43: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5036, rank=0.4737]\n",
            "Epoch 43/100\n",
            "  Loss: 0.5802\n",
            "  Ranking: 0.5580\n",
            "  Smoothness: 128.4979\n",
            "  Sparsity: 148.6939\n",
            "Epoch 44: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6034, rank=0.5923]\n",
            "Epoch 44/100\n",
            "  Loss: 0.5693\n",
            "  Ranking: 0.5478\n",
            "  Smoothness: 122.3998\n",
            "  Sparsity: 146.2280\n",
            "  Saved checkpoint: checkpoints/epoch_44.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 45: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.3703, rank=0.3453]\n",
            "Epoch 45/100\n",
            "  Loss: 0.5636\n",
            "  Ranking: 0.5416\n",
            "  Smoothness: 128.6387\n",
            "  Sparsity: 146.6771\n",
            "  Saved checkpoint: checkpoints/epoch_45.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 46: 100% 26/26 [01:02<00:00,  2.42s/it, loss=0.5720, rank=0.5385]\n",
            "Epoch 46/100\n",
            "  Loss: 0.5661\n",
            "  Ranking: 0.5424\n",
            "  Smoothness: 138.6926\n",
            "  Sparsity: 157.9200\n",
            "Epoch 47: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5602, rank=0.5339]\n",
            "Epoch 47/100\n",
            "  Loss: 0.5746\n",
            "  Ranking: 0.5544\n",
            "  Smoothness: 118.4884\n",
            "  Sparsity: 133.9331\n",
            "Epoch 48: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.6212, rank=0.5797]\n",
            "Epoch 48/100\n",
            "  Loss: 0.5638\n",
            "  Ranking: 0.5401\n",
            "  Smoothness: 139.0865\n",
            "  Sparsity: 157.5940\n",
            "Epoch 49: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6162, rank=0.5924]\n",
            "Epoch 49/100\n",
            "  Loss: 0.5696\n",
            "  Ranking: 0.5465\n",
            "  Smoothness: 134.3533\n",
            "  Sparsity: 154.8768\n",
            "Epoch 50: 100% 26/26 [01:02<00:00,  2.42s/it, loss=0.5725, rank=0.5473]\n",
            "Epoch 50/100\n",
            "  Loss: 0.5486\n",
            "  Ranking: 0.5264\n",
            "  Smoothness: 129.2216\n",
            "  Sparsity: 148.1023\n",
            "  Saved checkpoint: checkpoints/epoch_50.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 51: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.5978, rank=0.5692]\n",
            "Epoch 51/100\n",
            "  Loss: 0.5699\n",
            "  Ranking: 0.5482\n",
            "  Smoothness: 127.1744\n",
            "  Sparsity: 143.8705\n",
            "Epoch 52: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.4805, rank=0.4553]\n",
            "Epoch 52/100\n",
            "  Loss: 0.5554\n",
            "  Ranking: 0.5316\n",
            "  Smoothness: 138.2718\n",
            "  Sparsity: 159.2413\n",
            "Epoch 53: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5521, rank=0.5293]\n",
            "Epoch 53/100\n",
            "  Loss: 0.5526\n",
            "  Ranking: 0.5300\n",
            "  Smoothness: 133.6638\n",
            "  Sparsity: 148.9966\n",
            "Epoch 54: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.5834, rank=0.5658]\n",
            "Epoch 54/100\n",
            "  Loss: 0.5427\n",
            "  Ranking: 0.5198\n",
            "  Smoothness: 135.4729\n",
            "  Sparsity: 151.2145\n",
            "  Saved checkpoint: checkpoints/epoch_54.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 55: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.5046, rank=0.4793]\n",
            "Epoch 55/100\n",
            "  Loss: 0.5510\n",
            "  Ranking: 0.5284\n",
            "  Smoothness: 131.5343\n",
            "  Sparsity: 151.8344\n",
            "Epoch 56: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.4192, rank=0.3926]\n",
            "Epoch 56/100\n",
            "  Loss: 0.5293\n",
            "  Ranking: 0.5074\n",
            "  Smoothness: 127.2962\n",
            "  Sparsity: 146.2089\n",
            "  Saved checkpoint: checkpoints/epoch_56.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 57: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.4728, rank=0.4474]\n",
            "Epoch 57/100\n",
            "  Loss: 0.5393\n",
            "  Ranking: 0.5152\n",
            "  Smoothness: 141.4539\n",
            "  Sparsity: 160.0566\n",
            "Epoch 58: 100% 26/26 [01:03<00:00,  2.45s/it, loss=0.7001, rank=0.6772]\n",
            "Epoch 58/100\n",
            "  Loss: 0.5394\n",
            "  Ranking: 0.5163\n",
            "  Smoothness: 134.1561\n",
            "  Sparsity: 154.3296\n",
            "Epoch 59: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6116, rank=0.5927]\n",
            "Epoch 59/100\n",
            "  Loss: 0.5347\n",
            "  Ranking: 0.5127\n",
            "  Smoothness: 128.6331\n",
            "  Sparsity: 145.7790\n",
            "Epoch 60: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.5679, rank=0.5434]\n",
            "Epoch 60/100\n",
            "  Loss: 0.5398\n",
            "  Ranking: 0.5171\n",
            "  Smoothness: 130.6235\n",
            "  Sparsity: 152.4059\n",
            "  Saved checkpoint: checkpoints/epoch_60.pth\n",
            "Epoch 61: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.7065, rank=0.6788]\n",
            "Epoch 61/100\n",
            "  Loss: 0.5463\n",
            "  Ranking: 0.5230\n",
            "  Smoothness: 135.4711\n",
            "  Sparsity: 155.1438\n",
            "Epoch 62: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.4727, rank=0.4450]\n",
            "Epoch 62/100\n",
            "  Loss: 0.5426\n",
            "  Ranking: 0.5194\n",
            "  Smoothness: 135.9220\n",
            "  Sparsity: 153.3207\n",
            "Epoch 63: 100% 26/26 [01:03<00:00,  2.45s/it, loss=0.4910, rank=0.4703]\n",
            "Epoch 63/100\n",
            "  Loss: 0.5272\n",
            "  Ranking: 0.5047\n",
            "  Smoothness: 132.9525\n",
            "  Sparsity: 147.3686\n",
            "  Saved checkpoint: checkpoints/epoch_63.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 64: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.5242, rank=0.5020]\n",
            "Epoch 64/100\n",
            "  Loss: 0.5311\n",
            "  Ranking: 0.5081\n",
            "  Smoothness: 135.2601\n",
            "  Sparsity: 152.4730\n",
            "Epoch 65: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.4625, rank=0.4335]\n",
            "Epoch 65/100\n",
            "  Loss: 0.5294\n",
            "  Ranking: 0.5062\n",
            "  Smoothness: 136.2093\n",
            "  Sparsity: 154.4044\n",
            "Epoch 66: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.4695, rank=0.4424]\n",
            "Epoch 66/100\n",
            "  Loss: 0.5241\n",
            "  Ranking: 0.5001\n",
            "  Smoothness: 140.2652\n",
            "  Sparsity: 160.3501\n",
            "  Saved checkpoint: checkpoints/epoch_66.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 67: 100% 26/26 [01:03<00:00,  2.46s/it, loss=0.4676, rank=0.4404]\n",
            "Epoch 67/100\n",
            "  Loss: 0.5156\n",
            "  Ranking: 0.4928\n",
            "  Smoothness: 134.6475\n",
            "  Sparsity: 150.5934\n",
            "  Saved checkpoint: checkpoints/epoch_67.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 68: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.4540, rank=0.4202]\n",
            "Epoch 68/100\n",
            "  Loss: 0.5316\n",
            "  Ranking: 0.5090\n",
            "  Smoothness: 134.1866\n",
            "  Sparsity: 148.2780\n",
            "Epoch 69: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5122, rank=0.4791]\n",
            "Epoch 69/100\n",
            "  Loss: 0.5297\n",
            "  Ranking: 0.5065\n",
            "  Smoothness: 136.3072\n",
            "  Sparsity: 153.6340\n",
            "Epoch 70: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.4565, rank=0.4346]\n",
            "Epoch 70/100\n",
            "  Loss: 0.5320\n",
            "  Ranking: 0.5090\n",
            "  Smoothness: 133.8432\n",
            "  Sparsity: 152.9909\n",
            "  Saved checkpoint: checkpoints/epoch_70.pth\n",
            "Epoch 71: 100% 26/26 [01:03<00:00,  2.46s/it, loss=0.4298, rank=0.3981]\n",
            "Epoch 71/100\n",
            "  Loss: 0.5278\n",
            "  Ranking: 0.5034\n",
            "  Smoothness: 143.3587\n",
            "  Sparsity: 161.5051\n",
            "Epoch 72: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.4702, rank=0.4414]\n",
            "Epoch 72/100\n",
            "  Loss: 0.5225\n",
            "  Ranking: 0.4984\n",
            "  Smoothness: 139.6555\n",
            "  Sparsity: 161.1159\n",
            "Epoch 73: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.4978, rank=0.4714]\n",
            "Epoch 73/100\n",
            "  Loss: 0.5095\n",
            "  Ranking: 0.4856\n",
            "  Smoothness: 141.5777\n",
            "  Sparsity: 157.7549\n",
            "  Saved checkpoint: checkpoints/epoch_73.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 74: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.5786, rank=0.5581]\n",
            "Epoch 74/100\n",
            "  Loss: 0.5223\n",
            "  Ranking: 0.4979\n",
            "  Smoothness: 144.6531\n",
            "  Sparsity: 160.4698\n",
            "Epoch 75: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.5411, rank=0.5200]\n",
            "Epoch 75/100\n",
            "  Loss: 0.5231\n",
            "  Ranking: 0.5008\n",
            "  Smoothness: 130.3724\n",
            "  Sparsity: 148.7985\n",
            "Epoch 76: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.5101, rank=0.4816]\n",
            "Epoch 76/100\n",
            "  Loss: 0.5355\n",
            "  Ranking: 0.5109\n",
            "  Smoothness: 144.6844\n",
            "  Sparsity: 162.9766\n",
            "Epoch 77: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4723, rank=0.4497]\n",
            "Epoch 77/100\n",
            "  Loss: 0.5212\n",
            "  Ranking: 0.4987\n",
            "  Smoothness: 130.8487\n",
            "  Sparsity: 150.1868\n",
            "Epoch 78: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.5154, rank=0.4910]\n",
            "Epoch 78/100\n",
            "  Loss: 0.5133\n",
            "  Ranking: 0.4902\n",
            "  Smoothness: 137.4416\n",
            "  Sparsity: 151.2586\n",
            "Epoch 79: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4178, rank=0.3869]\n",
            "Epoch 79/100\n",
            "  Loss: 0.5155\n",
            "  Ranking: 0.4924\n",
            "  Smoothness: 134.6952\n",
            "  Sparsity: 153.6635\n",
            "Epoch 80: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.5333, rank=0.5086]\n",
            "Epoch 80/100\n",
            "  Loss: 0.5122\n",
            "  Ranking: 0.4877\n",
            "  Smoothness: 145.1851\n",
            "  Sparsity: 162.1798\n",
            "  Saved checkpoint: checkpoints/epoch_80.pth\n",
            "Epoch 81: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4840, rank=0.4627]\n",
            "Epoch 81/100\n",
            "  Loss: 0.5188\n",
            "  Ranking: 0.4957\n",
            "  Smoothness: 136.6429\n",
            "  Sparsity: 152.1415\n",
            "Epoch 82: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.5929, rank=0.5760]\n",
            "Epoch 82/100\n",
            "  Loss: 0.5186\n",
            "  Ranking: 0.4942\n",
            "  Smoothness: 142.9788\n",
            "  Sparsity: 162.0212\n",
            "Epoch 83: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4665, rank=0.4495]\n",
            "Epoch 83/100\n",
            "  Loss: 0.5111\n",
            "  Ranking: 0.4873\n",
            "  Smoothness: 140.8630\n",
            "  Sparsity: 156.5246\n",
            "Epoch 84: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4442, rank=0.4216]\n",
            "Epoch 84/100\n",
            "  Loss: 0.5149\n",
            "  Ranking: 0.4904\n",
            "  Smoothness: 143.5894\n",
            "  Sparsity: 162.1733\n",
            "Epoch 85:  81% 21/26 [00:56<00:14,  2.99s/it, loss=0.5325, rank=0.5110]"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python train.py --config configs/default.yaml --no-wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA6dRmILhxXZ"
      },
      "source": [
        "## 11. Evaluate\n",
        "\n",
        "Evaluate trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ9N5V8khxXZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python evaluate.py \\\n",
        "    --config configs/default.yaml \\\n",
        "    --checkpoint checkpoints/best_model.pth \\\n",
        "    --temporal-annotation data/annotations/Temporal_Anomaly_Annotation_for_Testing_Videos.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nwS2O59hxXZ"
      },
      "source": [
        "## 12. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OlxpzFLhxXZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Display ROC curve\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "if os.path.exists('results/roc_curve.png'):\n",
        "    display(Image('results/roc_curve.png'))\n",
        "\n",
        "# Print evaluation results\n",
        "if os.path.exists('results/evaluation_summary.txt'):\n",
        "    !cat results/evaluation_summary.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQmURsohxXZ"
      },
      "source": [
        "## 13. (Optional) Save Results to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8V9kmCvhxXa"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Backup checkpoints and results to Drive\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data_distribution'\n",
        "!mkdir -p \"{DRIVE_DATA_PATH}/results\"\n",
        "!cp -r checkpoints \"{DRIVE_DATA_PATH}/\"\n",
        "!cp -r results \"{DRIVE_DATA_PATH}/\"\n",
        "\n",
        "print(\"Results saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My new model:\n",
        "LeakyReLU Activation Function:\n",
        "\n",
        "Before: The original model used the ReLU activation function.\n",
        "\n",
        "After: I replaced ReLU with LeakyReLU. This modification allows a small, non-zero gradient when the input is negative (by setting a small slope, negative_slope=0.01). This is beneficial because, with ReLU, some neurons can \"die\" during training (i.e., they stop updating because they always output 0 for negative inputs). LeakyReLU mitigates this problem by allowing small negative values to propagate.\n",
        "\n",
        "Why: LeakyReLU helps the model learn more efficiently, especially in deep networks where dead neurons can be a problem, improving the overall performance and training stability.\n",
        "\n",
        "Dropout Regularization:\n",
        "\n",
        "Before: The original model didn't have dropout regularization, which can lead to overfitting, especially with complex models and limited data.\n",
        "\n",
        "After: I added Dropout with a rate of 0.5 (50% chance of dropping a neuron during training).\n",
        "\n",
        "Why: Dropout helps prevent overfitting by randomly setting a fraction of input units to zero during each forward pass. This encourages the model to not rely too heavily on any single neuron, thus improving generalization to unseen data.\n",
        "\n",
        "Batch Normalization:\n",
        "\n",
        "Before: The original model didn't use Batch Normalization.\n",
        "\n",
        "After: I added Batch Normalization after each fully connected layer (FC1 and FC2). This normalizes the output of each layer to have zero mean and unit variance, which stabilizes and speeds up training.\n",
        "\n",
        "Why: Batch Normalization helps the model learn faster by reducing internal covariate shift, which can lead to better performance, especially in deeper networks.\n",
        "\n",
        "Network Architecture Changes:\n",
        "\n",
        "Before: The original model had a simpler architecture with just two fully connected layers: 4096 -> 512 and 512 -> 1.\n",
        "\n",
        "After: I added a third fully connected layer with 512 -> 64 neurons. This gives the model an additional layer of abstraction, which could help it learn more complex patterns.\n",
        "\n",
        "Why: Increasing the depth of the network allows the model to learn more complex representations of the data. With the additional layer, the network can model finer-grained relationships between input features, which should improve its ability to detect anomalies.\n",
        "\n",
        "Sigmoid Activation:\n",
        "\n",
        "Before: The original model used a Sigmoid function at the output layer, which is still maintained in the new model.\n",
        "\n",
        "Why: The Sigmoid activation is useful for binary classification tasks (0 or 1), where we want the output to represent the probability of an anomaly. Since this is an anomaly detection task, the Sigmoid activation ensures that the model outputs a value between 0 and 1, representing the likelihood of each segment being an anomaly."
      ],
      "metadata": {
        "id": "G_0aR1z_ikpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ImprovedAnomalyDetector(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved 3-layer fully connected network for anomaly detection with dropout, batch normalization, and LeakyReLU.\n",
        "    Architecture:\n",
        "    - FC1: 4096 -> 512 (LeakyReLU + BatchNorm + Dropout 0.5)\n",
        "    - FC2: 512 -> 64 (LeakyReLU + BatchNorm + Dropout 0.5)\n",
        "    - FC3: 64 -> 1 (Sigmoid)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4096, dropout=0.5):\n",
        "        super(ImprovedAnomalyDetector, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, num_segments, feature_dim)\n",
        "                e.g., (30, 32, 4096)\n",
        "\n",
        "        Returns:\n",
        "            scores: (batch_size, num_segments)\n",
        "                    Anomaly score for each segment (0~1)\n",
        "        \"\"\"\n",
        "        batch_size, num_segments, feature_dim = x.shape\n",
        "\n",
        "        # Reshape to process all segments at once\n",
        "        x = x.view(-1, feature_dim)  # (batch_size * num_segments, 4096)\n",
        "\n",
        "        # FC layers with BatchNorm, LeakyReLU, and Dropout\n",
        "        x = self.fc1(x)           # (batch_size * num_segments, 512)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)           # (batch_size * num_segments, 64)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)           # (batch_size * num_segments, 1)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        # Reshape back\n",
        "        scores = x.view(batch_size, num_segments)  # (batch_size, 32)\n",
        "\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "lVDAetGgij-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Regularization of Smoothness and Sparsity with Alpha:\n",
        "\n",
        "Before: In the original MILRankingLoss class, the smoothness and sparsity constraints were used directly in the loss function. The weights for these constraints, λ1 (smoothness) and λ2 (sparsity), were defined, but there was no control over how much these two components influenced the total loss.\n",
        "\n",
        "After: I added a new parameter, alpha, which scales the contributions of the smoothness and sparsity terms. This allows us to have more fine-grained control over how much these constraints should affect the final loss.\n",
        "\n",
        "Why: The regularization factor (alpha) allows you to balance the importance of sparsity and smoothness in the overall model. This flexibility can be useful for optimizing the model's performance when you want to adjust how much these terms contribute to the loss function.\n",
        "\n",
        "\n",
        "3. Increased Focus on Sparsity and Smoothness:\n",
        "\n",
        "Before: The original loss computed sparsity as the sum of positive scores, which encouraged the model to focus on fewer segments that contribute significantly to the anomaly.\n",
        "\n",
        "After: The sparsity component is still the sum of positive scores, but the loss now has more control over this term, thanks to the alpha factor.\n",
        "\n",
        "Why: The sparsity loss encourages the model to focus on isolated, sparse anomalies rather than detecting large portions of the video as anomalies. By incorporating alpha, this constraint can be adjusted to affect the training process more or less strongly.\n",
        "\n",
        "4. Smoothing Between Segments:\n",
        "\n",
        "Before: In the original loss, smoothness was computed as the sum of squared differences between adjacent segments, but it didn't scale with any other hyperparameters.\n",
        "\n",
        "After: The smoothness penalty is now controlled by lambda1, and alpha scales the overall contribution of the smoothness loss. This enhances the control we have over how much temporal consistency is encouraged in the model’s predictions.\n",
        "\n",
        "Why: The smoothness loss ensures that anomalies detected by the model are coherent over time, which is important for tasks like video anomaly detection. By adding the alpha factor, we gain more control over the smoothness constraint, allowing the model to balance consistency and sparsity more effectively.\n",
        "\n",
        "5. Return of Detailed Loss Components:\n",
        "\n",
        "Before: The original loss function only returned the total loss without any breakdown of individual components.\n",
        "\n",
        "After: The detailed loss components (ranking loss, smoothness loss, and sparsity loss) are now returned in a dictionary.\n",
        "\n",
        "Why: Returning these components allows for better monitoring and debugging during training. You can now track how each term contributes to the final loss and adjust the hyperparameters (λ1, λ2, and alpha) accordingly during training."
      ],
      "metadata": {
        "id": "vlDkd_OSybrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedMILRankingLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple Instance Learning Ranking Loss with sparsity and smoothness constraints.\n",
        "\n",
        "    Loss formula from paper:\n",
        "    loss = hinge_loss + λ1 * smoothness + λ2 * sparsity\n",
        "\n",
        "    where:\n",
        "    - hinge_loss = max(0, 1 - max(pos_scores) + max(neg_scores))\n",
        "    - smoothness = sum of squared differences between adjacent segments\n",
        "    - sparsity = sum of all positive bag scores\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambda1=0.00008, lambda2=0.00008, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lambda1: weight for temporal smoothness constraint\n",
        "            lambda2: weight for sparsity constraint\n",
        "            alpha: factor to control how much sparsity and smoothness contribute to the final loss\n",
        "        \"\"\"\n",
        "        super(ImprovedMILRankingLoss, self).__init__()\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "        self.alpha = alpha  # Regularization factor for sparsity and smoothness terms\n",
        "\n",
        "    def forward(self, pos_scores, neg_scores):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pos_scores: (batch_pos, num_segments) - scores for positive bags\n",
        "            neg_scores: (batch_neg, num_segments) - scores for negative bags\n",
        "\n",
        "        Returns:\n",
        "            loss: scalar tensor\n",
        "        \"\"\"\n",
        "        # MIL ranking loss: max score of positive bag should be higher than negative\n",
        "        pos_max = torch.max(pos_scores, dim=1)[0]  # (batch_pos,)\n",
        "        neg_max = torch.max(neg_scores, dim=1)[0]  # (batch_neg,)\n",
        "\n",
        "        # Hinge loss\n",
        "        ranking_loss = torch.clamp(\n",
        "            1.0 - pos_max.mean() + neg_max.mean(),\n",
        "            min=0\n",
        "        )\n",
        "\n",
        "        # Temporal smoothness: minimize difference between adjacent segments\n",
        "        smoothness_loss = 0\n",
        "        if pos_scores.size(1) > 1:  # if more than 1 segment\n",
        "            temporal_diff = pos_scores[:, 1:] - pos_scores[:, :-1]  # (batch, 31)\n",
        "            smoothness_loss = torch.sum(temporal_diff ** 2)\n",
        "\n",
        "        # Sparsity: minimize sum of all scores (encourage sparse anomalies)\n",
        "        sparsity_loss = torch.sum(pos_scores)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = ranking_loss + self.lambda1 * smoothness_loss + self.lambda2 * sparsity_loss\n",
        "\n",
        "        total_loss += self.alpha * (self.lambda1 * smoothness_loss + self.lambda2 * sparsity_loss)\n",
        "\n",
        "        return total_loss, {\n",
        "            'ranking_loss': ranking_loss.item(),\n",
        "            'smoothness_loss': smoothness_loss.item() if isinstance(smoothness_loss, torch.Tensor) else 0.0,\n",
        "            'sparsity_loss': sparsity_loss.item()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "BVZFIF82iqcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Learning Rate Scheduling:\n",
        "\n",
        "Before: The learning rate remained constant throughout training, which could limit the model's ability to adapt as training progresses.\n",
        "\n",
        "After: I added a learning rate scheduler (StepLR) to decay the learning rate by 30% every 10 epochs.\n",
        "\n",
        "Why: A learning rate scheduler helps the model converge more efficiently by lowering the learning rate as training progresses. This enables finer adjustments to the model's weights as it approaches the optimal solution."
      ],
      "metadata": {
        "id": "8Cz6fj7ozqmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def build_model(config, device):\n",
        "    \"\"\"Build model and move to device\"\"\"\n",
        "    model = ImprovedAnomalyDetector(\n",
        "        input_dim=config['model']['input_dim'],\n",
        "        dropout=config['model']['dropout']\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_optimizer(model, config):\n",
        "    \"\"\"Build optimizer\"\"\"\n",
        "    optimizer_name = config['training']['optimizer'].lower()\n",
        "    lr = config['training']['learning_rate']\n",
        "    weight_decay = config['training']['lambda3']\n",
        "\n",
        "    if optimizer_name == 'adamw':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_ranking_loss = 0.0\n",
        "    epoch_smoothness_loss = 0.0\n",
        "    epoch_sparsity_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        pos_features = batch['pos_features']\n",
        "        neg_features = batch['neg_features']\n",
        "\n",
        "        if pos_features is None or neg_features is None:\n",
        "            continue\n",
        "\n",
        "        pos_features = pos_features.to(device)\n",
        "        neg_features = neg_features.to(device)\n",
        "\n",
        "        # Forward\n",
        "        pos_scores = model(pos_features)\n",
        "        neg_scores = model(neg_features)\n",
        "\n",
        "        # Loss\n",
        "        loss, loss_dict = criterion(pos_scores, neg_scores)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_ranking_loss += loss_dict['ranking_loss']\n",
        "        epoch_smoothness_loss += loss_dict['smoothness_loss']\n",
        "        epoch_sparsity_loss += loss_dict['sparsity_loss']\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'rank': f\"{loss_dict['ranking_loss']:.4f}\"\n",
        "        })\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    avg_ranking = epoch_ranking_loss / num_batches\n",
        "    avg_smoothness = epoch_smoothness_loss / num_batches\n",
        "    avg_sparsity = epoch_sparsity_loss / num_batches\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'ranking_loss': avg_ranking,\n",
        "        'smoothness_loss': avg_smoothness,\n",
        "        'sparsity_loss': avg_sparsity\n",
        "    }\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, save_path):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = Path('checkpoints/New Model')\n",
        "    checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Build dataset\n",
        "    train_dataset = C3DFeatureDataset(\n",
        "        annotation_path=config['data']['train_annotation_path'],\n",
        "        features_root=config['data']['feature_path']\n",
        "    )\n",
        "    print(f\"Train dataset: {len(train_dataset)} videos\")\n",
        "\n",
        "    # Build sampler and loader\n",
        "    sampler = BalancedBatchSampler(\n",
        "        train_dataset,\n",
        "        batch_size=config['training']['batch_size']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_sampler=sampler,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=4\n",
        "    )\n",
        "    print(f\"Total batches per epoch: {len(train_loader)}\")\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(config, device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Build optimizer\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    print(f\"Optimizer: {config['training']['optimizer']}\")\n",
        "\n",
        "    # Build loss\n",
        "    criterion = MILRankingLoss(\n",
        "        lambda1=config['training']['lambda1'],\n",
        "        lambda2=config['training']['lambda2']\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = config['training']['num_epochs']\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        metrics = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "        print(f\"  Ranking: {metrics['ranking_loss']:.4f}\")\n",
        "        print(f\"  Smoothness: {metrics['smoothness_loss']:.4f}\")\n",
        "        print(f\"  Sparsity: {metrics['sparsity_loss']:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch % 10 == 0 or metrics['loss'] < best_loss:\n",
        "            save_path = checkpoint_dir / f'epoch_{epoch}.pth'\n",
        "            save_checkpoint(model, optimizer, epoch, metrics['loss'], save_path)\n",
        "            print(f\"  Saved checkpoint: {save_path}\")\n",
        "\n",
        "            if metrics['loss'] < best_loss:\n",
        "                best_loss = metrics['loss']\n",
        "                best_path = checkpoint_dir / 'best_model.pth'\n",
        "                save_checkpoint(model, optimizer, epoch, metrics['loss'], best_path)\n",
        "                print(f\"  New best model: {best_path}\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'model': {'input_dim': 4096, 'dropout': 0.5},\n",
        "        'training': {'optimizer': 'adamw', 'learning_rate': 1e-4, 'lambda3': 0.01, 'num_epochs': 50, 'batch_size': 32, 'lambda1': 0.00008, 'lambda2': 0.00008},\n",
        "        'data': {'train_annotation_path': 'path_to_annotations', 'feature_path': 'path_to_features'}\n",
        "    }\n",
        "    main(config)\n"
      ],
      "metadata": {
        "id": "4-GlZrA6lC_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Precision-Recall Curve:\n",
        "\n",
        "Before: Precision-Recall (PR) curve not have been included.\n",
        "\n",
        "After: I added the plot_pr_curve function to calculate and save the PR curve and its AUC for further evaluation.\n",
        "\n",
        "Why: The PR curve is especially useful in imbalanced datasets like anomaly detection, where the number of normal segments greatly outweighs anomalies. The PR AUC gives a better sense of model performance in these cases."
      ],
      "metadata": {
        "id": "XEx48R6Z0aIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def load_temporal_annotations(annotation_file):\n",
        "    \"\"\"Load temporal annotations for test videos.\"\"\"\n",
        "    annotations = {}\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 6:\n",
        "                continue\n",
        "            video_name = parts[0].replace('.mp4', '')\n",
        "            start1, end1 = int(parts[2]), int(parts[3])\n",
        "            start2, end2 = int(parts[4]), int(parts[5])\n",
        "            segments = []\n",
        "            if start1 != -1 and end1 != -1:\n",
        "                segments.append((start1, end1))\n",
        "            if start2 != -1 and end2 != -1:\n",
        "                segments.append((start2, end2))\n",
        "            annotations[video_name] = segments\n",
        "    return annotations\n",
        "\n",
        "\n",
        "def get_frame_level_labels(video_name, annotations, num_segments=32, fps=30):\n",
        "    \"\"\"Generate binary labels for video segments (0 = normal, 1 = anomaly).\"\"\"\n",
        "    labels = np.zeros(num_segments, dtype=np.int32)\n",
        "    base_name = video_name.split('/')[-1]\n",
        "    if base_name not in annotations:\n",
        "        return labels\n",
        "    anomaly_segments = annotations[base_name]\n",
        "    if not anomaly_segments:\n",
        "        return labels\n",
        "    max_frame = max(end for _, end in anomaly_segments)\n",
        "    frames_per_segment = max_frame / num_segments\n",
        "    for seg_idx in range(num_segments):\n",
        "        seg_start = seg_idx * frames_per_segment\n",
        "        seg_end = (seg_idx + 1) * frames_per_segment\n",
        "        for anomaly_start, anomaly_end in anomaly_segments:\n",
        "            if not (seg_end < anomaly_start or seg_start > anomaly_end):\n",
        "                labels[seg_idx] = 1\n",
        "                break\n",
        "    return labels\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset, annotations, device):\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n",
        "            sample = dataset[idx]\n",
        "            features = sample['features'].unsqueeze(0).to(device)  # (1, 32, 4096)\n",
        "            video_name = sample['video_name']\n",
        "\n",
        "            # Get predictions\n",
        "            scores = model(features).squeeze(0).cpu().numpy()  # (32,)\n",
        "\n",
        "            # Get ground truth labels\n",
        "            labels = get_frame_level_labels(video_name, annotations)\n",
        "\n",
        "            all_labels.extend(labels)\n",
        "            all_scores.extend(scores)\n",
        "\n",
        "    return np.array(all_labels), np.array(all_scores)\n",
        "\n",
        "\n",
        "def plot_roc_curve(labels, scores, save_path):\n",
        "    \"\"\"Plot and save ROC curve\"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"ROC curve saved to {save_path}\")\n",
        "\n",
        "    return roc_auc, fpr, tpr, thresholds\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def plot_pr_curve(labels, scores, save_path):\n",
        "    precision, recall, _ = precision_recall_curve(labels, scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"PR curve saved to {save_path}\")\n",
        "\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "def save_results(labels, scores, save_path):\n",
        "    \"\"\"Save evaluation results\"\"\"\n",
        "    results = {\n",
        "        'labels': labels.tolist(),\n",
        "        'scores': scores.tolist()\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"Results saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Directly define the paths\n",
        "new_model_checkpoint = \"checkpoints/New Model/best_model.pth\"\n",
        "temporal_annotation_path = \"data/annotations/test_set.txt\"\n",
        "\n",
        "# Define the config dictionary directly\n",
        "config = {\n",
        "    'model': {\n",
        "        'input_dim': 4096,\n",
        "        'dropout': 0.5\n",
        "    },\n",
        "    'training': {\n",
        "        'optimizer': 'adamw',\n",
        "        'learning_rate': 1e-4,\n",
        "        'lambda3': 0.01,\n",
        "        'num_epochs': 20,\n",
        "        'batch_size': 32,\n",
        "        'lambda1': 0.00008,\n",
        "        'lambda2': 0.00008\n",
        "    },\n",
        "    'data': {\n",
        "        'test_annotation_path': \"data/annotations/test_set.txt\",  # Using the test annotation path\n",
        "        'feature_path': \"data/features\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load new model\n",
        "new_model = ImprovedAnomalyDetector(\n",
        "    input_dim=config['model']['input_dim'],\n",
        "    dropout=config['model']['dropout']\n",
        ")\n",
        "\n",
        "# Load new model checkpoint\n",
        "new_model_checkpoint = torch.load(new_model_checkpoint, map_location=device)\n",
        "new_model.load_state_dict(new_model_checkpoint['model_state_dict'])\n",
        "new_model = new_model.to(device)\n",
        "\n",
        "print(f\"Loaded new model from {new_model_checkpoint}\")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = C3DFeatureDataset(\n",
        "    annotation_path=config['data']['test_annotation_path'],\n",
        "    features_root=config['data']['feature_path']\n",
        ")\n",
        "\n",
        "print(f\"Test dataset: {len(test_dataset)} videos\")\n",
        "\n",
        "# Load temporal annotations\n",
        "annotations = load_temporal_annotations(temporal_annotation_path)\n",
        "print(f\"Loaded temporal annotations for {len(annotations)} videos\")\n",
        "\n",
        "# Evaluate new model\n",
        "print(\"\\nEvaluating new model...\")\n",
        "new_labels, new_scores = evaluate_model(new_model, test_dataset, annotations, device)\n",
        "\n",
        "# Calculate ROC curve\n",
        "roc_auc, fpr, tpr, thresholds = plot_roc_curve(new_labels, new_scores, 'results/new_model_roc_curve.png')\n",
        "\n",
        " # Plot and save PR curve\n",
        "    pr_auc = plot_pr_curve(\n",
        "        new_labels, new_scores,\n",
        "       'results/new_model_pr_curve.png'\n",
        "    )\n",
        "\n",
        "# Save results for new model\n",
        "save_results(new_labels, new_scores, 'results/new_model_evaluation_results.json')\n",
        "\n",
        "# Summary of evaluation results\n",
        "print(f\"\\nTotal segments evaluated: {len(new_labels)}\")\n",
        "print(f\"Anomaly segments: {new_labels.sum()} ({new_labels.sum()/len(new_labels)*100:.1f}%)\")\n",
        "print(f\"Normal segments: {len(new_labels) - new_labels.sum()} ({(len(new_labels)-new_labels.sum())/len(new_labels)*100:.1f}%)\")\n",
        "\n",
        "# Find optimal threshold (Youden's J statistic)\n",
        "j_scores = tpr - fpr\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(f\"\\nOptimal threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"  TPR: {tpr[optimal_idx]:.4f}\")\n",
        "print(f\"  FPR: {fpr[optimal_idx]:.4f}\")\n",
        "\n",
        "# Save summary\n",
        "results_dir = Path('results/New Model')\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "summary_path = results_dir / 'evaluation_summary.txt'\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(f\"Evaluation Summary\\n\")\n",
        "    f.write(f\"{'='*60}\\n\")\n",
        "    f.write(f\"Model: {new_model_checkpoint}\\n\")\n",
        "    f.write(f\"Test videos: {len(test_dataset)}\\n\")\n",
        "    f.write(f\"Total segments: {len(new_labels)}\\n\")\n",
        "    f.write(f\"Anomaly segments: {new_labels.sum()} ({new_labels.sum()/len(new_labels)*100:.1f}%)\\n\")\n",
        "    f.write(f\"\\nResults:\\n\")\n",
        "    f.write(f\"  AUC: {roc_auc:.4f}\\n\")\n",
        "    f.write(f\"  Optimal threshold: {optimal_threshold:.4f}\\n\")\n",
        "    f.write(f\"  TPR at optimal: {tpr[optimal_idx]:.4f}\\n\")\n",
        "    f.write(f\"  FPR at optimal: {fpr[optimal_idx]:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nSummary saved to {summary_path}\")\n"
      ],
      "metadata": {
        "id": "PZMrGRyOl0kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define the file paths for both models\n",
        "baseline_roc_path = 'results/roc_curve.png'\n",
        "new_model_roc_path = 'results/new_model_roc_curve.png'\n",
        "\n",
        "# Display ROC curve for baseline model\n",
        "if os.path.exists(baseline_roc_path):\n",
        "    print(\"Displaying ROC curve for Baseline Model:\")\n",
        "    display(Image(baseline_roc_path))\n",
        "\n",
        "# Display ROC curve for new model\n",
        "if os.path.exists(new_model_roc_path):\n",
        "    print(\"Displaying ROC curve for New Model:\")\n",
        "    display(Image(new_model_roc_path))\n",
        "\n",
        "# Define the file paths for evaluation results of both models\n",
        "baseline_eval_results_path = 'results/evaluation_results.json'\n",
        "new_model_eval_results_path = 'results/new_model_evaluation_results.json'\n",
        "\n",
        "# Display evaluation results for baseline model\n",
        "if os.path.exists(baseline_eval_results_path):\n",
        "    print(\"\\nEvaluation results for Baseline Model:\")\n",
        "    with open(baseline_eval_results_path, 'r') as f:\n",
        "        baseline_results = json.load(f)\n",
        "    print(f\"Labels: {baseline_results['labels'][:5]}...\")\n",
        "    print(f\"Scores: {baseline_results['scores'][:5]}...\")\n",
        "# Display evaluation results for new model\n",
        "if os.path.exists(new_model_eval_results_path):\n",
        "    print(\"\\nEvaluation results for New Model:\")\n",
        "    with open(new_model_eval_results_path, 'r') as f:\n",
        "        new_model_results = json.load(f)\n",
        "    print(f\"Labels: {new_model_results['labels'][:5]}...\")\n",
        "    print(f\"Scores: {new_model_results['scores'][:5]}...\")\n"
      ],
      "metadata": {
        "id": "JUI7vGnPnLCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}