{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_WpWke0hxXN"
      },
      "source": [
        "# UCF-Crime Anomaly Detection - Colab Setup\n",
        "\n",
        "Setup notebook for running MIL Ranking Loss re-implementation on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn-H35qBhxXP"
      },
      "source": [
        "## 1. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN-HxfyThxXQ",
        "outputId": "7c482a08-c147-4d8a-be98-1203078086bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 30 11:31:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              8W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "PyTorch CUDA available: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# Verify PyTorch can use GPU\n",
        "import torch\n",
        "print(f\"\\nPyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WNVNwnhxXS"
      },
      "source": [
        "## 2. Mount Google Drive\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Upload features.zip and annotations.zip to Google Drive\n",
        "2. Google Drive structure:\n",
        "```\n",
        "MyDrive/\n",
        "└── Colab Notebooks/\n",
        "    └── data_distribution/\n",
        "        ├── features.zip\n",
        "        └── annotations.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDQpbfvdhxXT",
        "outputId": "d8c6867f-4d2b-452f-e1af-19f2463fa573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8LF-NwXhxXU"
      },
      "source": [
        "## 3. Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjIMc96hxXU"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/KwonPodo/MILRankingLoss_Sultani2018_ReImplementation.git\n",
        "%cd MILRankingLoss_Sultani2018_ReImplementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzjvGTh1hxXV"
      },
      "source": [
        "## 4. Install Packages\n",
        "\n",
        "**Note:** Using `requirements-colab.txt` to avoid package conflicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrGcuM45hxXV",
        "outputId": "e26f213e-1042-4916-f919-fab2aeb46bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "NumPy: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install minimal packages for Colab (avoid conflicts)\n",
        "!pip install -r requirements-colab.txt -q\n",
        "\n",
        "# Check installed package versions\n",
        "import torch\n",
        "import numpy as np\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrp7JgohxXV"
      },
      "source": [
        "## 5. Extract Data\n",
        "\n",
        "Extract features from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og3JPsgnhxXW",
        "outputId": "040d5c74-f3d3-461b-c837-cbdc5e6d4ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "replace data/features/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Features extracted\n",
            "total 4.0K\n",
            "drwxr-xr-x 17 root root 4.0K Nov 30 11:31 features\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Verify working directory\n",
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!pwd\n",
        "\n",
        "# Create data directory\n",
        "!mkdir -p data\n",
        "\n",
        "# Google Drive path\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data_distribution'\n",
        "\n",
        "# Extract features\n",
        "!cp \"{DRIVE_DATA_PATH}/features.zip\" data/\n",
        "!unzip -q data/features.zip -d data/\n",
        "!rm data/features.zip\n",
        "\n",
        "print(\"Features extracted\")\n",
        "!ls -lh data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34-OkYIShxXW"
      },
      "source": [
        "## 6. Extract Annotations\n",
        "\n",
        "Extract annotation files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdKZqOuphxXX",
        "outputId": "bb5ae48c-828d-4ee9-dade-88cb0539a230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations extracted\n",
            "total 100K\n",
            "-rwxr-xr-x 1 root root 16K Jan  3  2023 Temporal_Anomaly_Annotation_for_Testing_Videos.txt\n",
            "-rwxr-xr-x 1 root root 13K Oct 31 17:28 test_set.txt\n",
            "-rwxr-xr-x 1 root root 66K Oct 31 17:28 train_set.txt\n"
          ]
        }
      ],
      "source": [
        "# Extract annotations from Google Drive\n",
        "!cp \"{DRIVE_DATA_PATH}/annotations.zip\" data/\n",
        "!unzip -q data/annotations.zip -d data/\n",
        "!rm data/annotations.zip\n",
        "\n",
        "print(\"Annotations extracted\")\n",
        "!ls -lh data/annotations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Y4lj_uhxXX"
      },
      "source": [
        "## 7. Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBZl7KnnhxXX",
        "outputId": "606d35ad-80b3-4cf8-cdac-213cdbf25ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Abuse\tAssault    Fighting\t  Shooting     Testing_Normal_Videos_Anomaly\n",
            "Arrest\tBurglary   RoadAccidents  Shoplifting  Training_Normal_Videos_Anomaly\n",
            "Arson\tExplosion  Robbery\t  Stealing     Vandalism\n",
            "Train samples:\n",
            "1610 data/annotations/train_set.txt\n",
            "Test samples:\n",
            "290 data/annotations/test_set.txt\n"
          ]
        }
      ],
      "source": [
        "# Verify working directory\n",
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Check feature categories\n",
        "!ls data/features/\n",
        "\n",
        "# Check sample counts\n",
        "!echo \"Train samples:\"\n",
        "!wc -l data/annotations/train_set.txt\n",
        "!echo \"Test samples:\"\n",
        "!wc -l data/annotations/test_set.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXD8k0kEhxXY"
      },
      "source": [
        "## 8. Test Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4NiamhghxXY",
        "outputId": "e747739c-467a-4016-f93c-e0d387ec1369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Total samples in dataset: 1610\n",
            "Positive samples: 810\n",
            "Negative samples: 800\n",
            "\n",
            "First batch:\n",
            "\n",
            "Positive bags: torch.Size([30, 32, 4096])\n",
            "\n",
            "Negative bags: torch.Size([30, 32, 4096])\n",
            "Batch 0: pos=30, neg=30\n",
            "Batch 1: pos=30, neg=30\n",
            "Batch 2: pos=30, neg=30\n",
            "Batch 3: pos=30, neg=30\n"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python scripts/test_dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Ho4KP6hxXY"
      },
      "source": [
        "## 9. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg155EkHhxXY",
        "outputId": "fb3becd6-1924-4c6e-ec54-dc9b1c355d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Model architecture:\n",
            "AnomalyDetector(\n",
            "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.6, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Total parameters: 2,114,113\n",
            "\n",
            "Positive scores shape: torch.Size([30, 32])\n",
            "Negative scores shape: torch.Size([30, 32])\n",
            "Score range: [0.4717, 0.5796]\n",
            "\n",
            "Total loss: 1.0406\n",
            "  Ranking loss: 0.9999\n",
            "  Smoothness loss: 0.6181\n",
            "  Sparsity loss: 507.8174\n",
            "\n",
            "Training mode loss: 1.0406\n"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python scripts/test_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydP3YZTFhxXY"
      },
      "source": [
        "## 10. Start Training\n",
        "\n",
        "### Option 1: Train without WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHgnmRshhxXY",
        "outputId": "e955425d-cf5f-4feb-f6df-0803188c47c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MILRankingLoss_Sultani2018_ReImplementation\n",
            "Loaded config from configs/default.yaml\n",
            "Using device: cuda\n",
            "Train dataset: 1610 videos\n",
            "Positive samples: 810\n",
            "Negative samples: 800\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Total batches per epoch: 26\n",
            "Model parameters: 2,114,113\n",
            "Optimizer: adam\n",
            "\n",
            "Starting training for 100 epochs...\n",
            "Epoch 1:   0% 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100% 26/26 [01:04<00:00,  2.47s/it, loss=1.0375, rank=0.9993]\n",
            "Epoch 1/100\n",
            "  Loss: 1.0376\n",
            "  Ranking: 0.9991\n",
            "  Smoothness: 0.0337\n",
            "  Sparsity: 480.9726\n",
            "  Saved checkpoint: checkpoints/epoch_1.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 2: 100% 26/26 [01:02<00:00,  2.42s/it, loss=1.0351, rank=0.9974]\n",
            "Epoch 2/100\n",
            "  Loss: 1.0365\n",
            "  Ranking: 0.9986\n",
            "  Smoothness: 0.0316\n",
            "  Sparsity: 474.5381\n",
            "  Saved checkpoint: checkpoints/epoch_2.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 3: 100% 26/26 [01:01<00:00,  2.35s/it, loss=1.0353, rank=0.9981]\n",
            "Epoch 3/100\n",
            "  Loss: 1.0358\n",
            "  Ranking: 0.9983\n",
            "  Smoothness: 0.0395\n",
            "  Sparsity: 468.5425\n",
            "  Saved checkpoint: checkpoints/epoch_3.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 4: 100% 26/26 [01:02<00:00,  2.41s/it, loss=1.0327, rank=0.9956]\n",
            "Epoch 4/100\n",
            "  Loss: 1.0336\n",
            "  Ranking: 0.9965\n",
            "  Smoothness: 0.0889\n",
            "  Sparsity: 463.8808\n",
            "  Saved checkpoint: checkpoints/epoch_4.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 5: 100% 26/26 [01:02<00:00,  2.40s/it, loss=1.0183, rank=0.9813]\n",
            "Epoch 5/100\n",
            "  Loss: 1.0272\n",
            "  Ranking: 0.9901\n",
            "  Smoothness: 0.3165\n",
            "  Sparsity: 462.7753\n",
            "  Saved checkpoint: checkpoints/epoch_5.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 6: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.9944, rank=0.9578]\n",
            "Epoch 6/100\n",
            "  Loss: 1.0140\n",
            "  Ranking: 0.9767\n",
            "  Smoothness: 1.5048\n",
            "  Sparsity: 464.6150\n",
            "  Saved checkpoint: checkpoints/epoch_6.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 7: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.9626, rank=0.9260]\n",
            "Epoch 7/100\n",
            "  Loss: 0.9911\n",
            "  Ranking: 0.9539\n",
            "  Smoothness: 5.2399\n",
            "  Sparsity: 459.5910\n",
            "  Saved checkpoint: checkpoints/epoch_7.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 8: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.9144, rank=0.8775]\n",
            "Epoch 8/100\n",
            "  Loss: 0.9651\n",
            "  Ranking: 0.9281\n",
            "  Smoothness: 10.1931\n",
            "  Sparsity: 451.7853\n",
            "  Saved checkpoint: checkpoints/epoch_8.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 9: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.9314, rank=0.8963]\n",
            "Epoch 9/100\n",
            "  Loss: 0.9322\n",
            "  Ranking: 0.8956\n",
            "  Smoothness: 18.1626\n",
            "  Sparsity: 438.9566\n",
            "  Saved checkpoint: checkpoints/epoch_9.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 10: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8542, rank=0.8198]\n",
            "Epoch 10/100\n",
            "  Loss: 0.9034\n",
            "  Ranking: 0.8680\n",
            "  Smoothness: 29.7606\n",
            "  Sparsity: 413.1727\n",
            "  Saved checkpoint: checkpoints/epoch_10.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 11: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.8499, rank=0.8179]\n",
            "Epoch 11/100\n",
            "  Loss: 0.8781\n",
            "  Ranking: 0.8436\n",
            "  Smoothness: 37.8653\n",
            "  Sparsity: 392.4199\n",
            "  Saved checkpoint: checkpoints/epoch_11.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 12: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8880, rank=0.8588]\n",
            "Epoch 12/100\n",
            "  Loss: 0.8498\n",
            "  Ranking: 0.8182\n",
            "  Smoothness: 50.7010\n",
            "  Sparsity: 343.5123\n",
            "  Saved checkpoint: checkpoints/epoch_12.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 13: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.8435, rank=0.8212]\n",
            "Epoch 13/100\n",
            "  Loss: 0.8257\n",
            "  Ranking: 0.7963\n",
            "  Smoothness: 60.3190\n",
            "  Sparsity: 307.6057\n",
            "  Saved checkpoint: checkpoints/epoch_13.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 14: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6930, rank=0.6697]\n",
            "Epoch 14/100\n",
            "  Loss: 0.7937\n",
            "  Ranking: 0.7665\n",
            "  Smoothness: 80.0147\n",
            "  Sparsity: 260.3102\n",
            "  Saved checkpoint: checkpoints/epoch_14.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 15: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.7425, rank=0.7229]\n",
            "Epoch 15/100\n",
            "  Loss: 0.7615\n",
            "  Ranking: 0.7385\n",
            "  Smoothness: 85.2982\n",
            "  Sparsity: 201.5434\n",
            "  Saved checkpoint: checkpoints/epoch_15.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 16: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.6957, rank=0.6751]\n",
            "Epoch 16/100\n",
            "  Loss: 0.7387\n",
            "  Ranking: 0.7181\n",
            "  Smoothness: 91.4944\n",
            "  Sparsity: 166.9637\n",
            "  Saved checkpoint: checkpoints/epoch_16.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 17: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.7162, rank=0.7007]\n",
            "Epoch 17/100\n",
            "  Loss: 0.7189\n",
            "  Ranking: 0.6989\n",
            "  Smoothness: 92.0162\n",
            "  Sparsity: 157.4646\n",
            "  Saved checkpoint: checkpoints/epoch_17.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 18: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6096, rank=0.5867]\n",
            "Epoch 18/100\n",
            "  Loss: 0.7157\n",
            "  Ranking: 0.6944\n",
            "  Smoothness: 104.8757\n",
            "  Sparsity: 161.5662\n",
            "  Saved checkpoint: checkpoints/epoch_18.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 19: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.7825, rank=0.7514]\n",
            "Epoch 19/100\n",
            "  Loss: 0.7049\n",
            "  Ranking: 0.6841\n",
            "  Smoothness: 104.6911\n",
            "  Sparsity: 155.8092\n",
            "  Saved checkpoint: checkpoints/epoch_19.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 20: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.7277, rank=0.6986]\n",
            "Epoch 20/100\n",
            "  Loss: 0.6931\n",
            "  Ranking: 0.6737\n",
            "  Smoothness: 98.4537\n",
            "  Sparsity: 144.0865\n",
            "  Saved checkpoint: checkpoints/epoch_20.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 21: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5977, rank=0.5737]\n",
            "Epoch 21/100\n",
            "  Loss: 0.6792\n",
            "  Ranking: 0.6583\n",
            "  Smoothness: 107.9645\n",
            "  Sparsity: 152.8774\n",
            "  Saved checkpoint: checkpoints/epoch_21.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 22: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.6999, rank=0.6803]\n",
            "Epoch 22/100\n",
            "  Loss: 0.6560\n",
            "  Ranking: 0.6362\n",
            "  Smoothness: 103.6883\n",
            "  Sparsity: 143.9402\n",
            "  Saved checkpoint: checkpoints/epoch_22.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 23: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.8039, rank=0.7922]\n",
            "Epoch 23/100\n",
            "  Loss: 0.6673\n",
            "  Ranking: 0.6494\n",
            "  Smoothness: 98.1108\n",
            "  Sparsity: 125.0259\n",
            "Epoch 24: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.6014, rank=0.5763]\n",
            "Epoch 24/100\n",
            "  Loss: 0.6570\n",
            "  Ranking: 0.6358\n",
            "  Smoothness: 115.8001\n",
            "  Sparsity: 149.4256\n",
            "Epoch 25: 100% 26/26 [01:03<00:00,  2.43s/it, loss=0.6301, rank=0.6111]\n",
            "Epoch 25/100\n",
            "  Loss: 0.6577\n",
            "  Ranking: 0.6363\n",
            "  Smoothness: 116.0188\n",
            "  Sparsity: 150.5183\n",
            "Epoch 26: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.6264, rank=0.5988]\n",
            "Epoch 26/100\n",
            "  Loss: 0.6268\n",
            "  Ranking: 0.6071\n",
            "  Smoothness: 110.1000\n",
            "  Sparsity: 136.6329\n",
            "  Saved checkpoint: checkpoints/epoch_26.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 27: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.5949, rank=0.5721]\n",
            "Epoch 27/100\n",
            "  Loss: 0.6409\n",
            "  Ranking: 0.6209\n",
            "  Smoothness: 108.1847\n",
            "  Sparsity: 141.5359\n",
            "Epoch 28: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.6831, rank=0.6614]\n",
            "Epoch 28/100\n",
            "  Loss: 0.6267\n",
            "  Ranking: 0.6053\n",
            "  Smoothness: 118.9159\n",
            "  Sparsity: 149.2820\n",
            "  Saved checkpoint: checkpoints/epoch_28.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 29: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.6882, rank=0.6698]\n",
            "Epoch 29/100\n",
            "  Loss: 0.6284\n",
            "  Ranking: 0.6096\n",
            "  Smoothness: 104.3121\n",
            "  Sparsity: 129.7739\n",
            "Epoch 30: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6020, rank=0.5874]\n",
            "Epoch 30/100\n",
            "  Loss: 0.6252\n",
            "  Ranking: 0.6044\n",
            "  Smoothness: 117.8821\n",
            "  Sparsity: 142.5926\n",
            "  Saved checkpoint: checkpoints/epoch_30.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 31: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5943, rank=0.5782]\n",
            "Epoch 31/100\n",
            "  Loss: 0.6176\n",
            "  Ranking: 0.5976\n",
            "  Smoothness: 113.0824\n",
            "  Sparsity: 137.0418\n",
            "  Saved checkpoint: checkpoints/epoch_31.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 32: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.5618, rank=0.5416]\n",
            "Epoch 32/100\n",
            "  Loss: 0.6138\n",
            "  Ranking: 0.5946\n",
            "  Smoothness: 109.4350\n",
            "  Sparsity: 130.9676\n",
            "  Saved checkpoint: checkpoints/epoch_32.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 33: 100% 26/26 [01:04<00:00,  2.46s/it, loss=0.5198, rank=0.5035]\n",
            "Epoch 33/100\n",
            "  Loss: 0.6084\n",
            "  Ranking: 0.5882\n",
            "  Smoothness: 115.1455\n",
            "  Sparsity: 136.5571\n",
            "  Saved checkpoint: checkpoints/epoch_33.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 34: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.6168, rank=0.5961]\n",
            "Epoch 34/100\n",
            "  Loss: 0.6057\n",
            "  Ranking: 0.5857\n",
            "  Smoothness: 114.3990\n",
            "  Sparsity: 134.9616\n",
            "  Saved checkpoint: checkpoints/epoch_34.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 35: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.7752, rank=0.7549]\n",
            "Epoch 35/100\n",
            "  Loss: 0.6028\n",
            "  Ranking: 0.5818\n",
            "  Smoothness: 118.6778\n",
            "  Sparsity: 142.9282\n",
            "  Saved checkpoint: checkpoints/epoch_35.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 36: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6980, rank=0.6772]\n",
            "Epoch 36/100\n",
            "  Loss: 0.5928\n",
            "  Ranking: 0.5707\n",
            "  Smoothness: 125.3819\n",
            "  Sparsity: 150.9625\n",
            "  Saved checkpoint: checkpoints/epoch_36.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 37: 100% 26/26 [01:03<00:00,  2.46s/it, loss=0.6767, rank=0.6568]\n",
            "Epoch 37/100\n",
            "  Loss: 0.5950\n",
            "  Ranking: 0.5717\n",
            "  Smoothness: 132.5752\n",
            "  Sparsity: 159.0052\n",
            "Epoch 38: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.6110, rank=0.5815]\n",
            "Epoch 38/100\n",
            "  Loss: 0.6076\n",
            "  Ranking: 0.5842\n",
            "  Smoothness: 134.4324\n",
            "  Sparsity: 158.4010\n",
            "Epoch 39: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.6224, rank=0.6088]\n",
            "Epoch 39/100\n",
            "  Loss: 0.5902\n",
            "  Ranking: 0.5677\n",
            "  Smoothness: 127.7802\n",
            "  Sparsity: 154.3882\n",
            "  Saved checkpoint: checkpoints/epoch_39.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 40: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.5098, rank=0.4892]\n",
            "Epoch 40/100\n",
            "  Loss: 0.5872\n",
            "  Ranking: 0.5670\n",
            "  Smoothness: 114.8599\n",
            "  Sparsity: 138.6194\n",
            "  Saved checkpoint: checkpoints/epoch_40.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 41: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5230, rank=0.4990]\n",
            "Epoch 41/100\n",
            "  Loss: 0.5917\n",
            "  Ranking: 0.5687\n",
            "  Smoothness: 133.1585\n",
            "  Sparsity: 155.1772\n",
            "Epoch 42: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.4875, rank=0.4697]\n",
            "Epoch 42/100\n",
            "  Loss: 0.5705\n",
            "  Ranking: 0.5492\n",
            "  Smoothness: 121.1375\n",
            "  Sparsity: 144.6272\n",
            "  Saved checkpoint: checkpoints/epoch_42.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 43: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5036, rank=0.4737]\n",
            "Epoch 43/100\n",
            "  Loss: 0.5802\n",
            "  Ranking: 0.5580\n",
            "  Smoothness: 128.4979\n",
            "  Sparsity: 148.6939\n",
            "Epoch 44: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6034, rank=0.5923]\n",
            "Epoch 44/100\n",
            "  Loss: 0.5693\n",
            "  Ranking: 0.5478\n",
            "  Smoothness: 122.3998\n",
            "  Sparsity: 146.2280\n",
            "  Saved checkpoint: checkpoints/epoch_44.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 45: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.3703, rank=0.3453]\n",
            "Epoch 45/100\n",
            "  Loss: 0.5636\n",
            "  Ranking: 0.5416\n",
            "  Smoothness: 128.6387\n",
            "  Sparsity: 146.6771\n",
            "  Saved checkpoint: checkpoints/epoch_45.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 46: 100% 26/26 [01:02<00:00,  2.42s/it, loss=0.5720, rank=0.5385]\n",
            "Epoch 46/100\n",
            "  Loss: 0.5661\n",
            "  Ranking: 0.5424\n",
            "  Smoothness: 138.6926\n",
            "  Sparsity: 157.9200\n",
            "Epoch 47: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5602, rank=0.5339]\n",
            "Epoch 47/100\n",
            "  Loss: 0.5746\n",
            "  Ranking: 0.5544\n",
            "  Smoothness: 118.4884\n",
            "  Sparsity: 133.9331\n",
            "Epoch 48: 100% 26/26 [01:01<00:00,  2.35s/it, loss=0.6212, rank=0.5797]\n",
            "Epoch 48/100\n",
            "  Loss: 0.5638\n",
            "  Ranking: 0.5401\n",
            "  Smoothness: 139.0865\n",
            "  Sparsity: 157.5940\n",
            "Epoch 49: 100% 26/26 [01:02<00:00,  2.41s/it, loss=0.6162, rank=0.5924]\n",
            "Epoch 49/100\n",
            "  Loss: 0.5696\n",
            "  Ranking: 0.5465\n",
            "  Smoothness: 134.3533\n",
            "  Sparsity: 154.8768\n",
            "Epoch 50: 100% 26/26 [01:02<00:00,  2.42s/it, loss=0.5725, rank=0.5473]\n",
            "Epoch 50/100\n",
            "  Loss: 0.5486\n",
            "  Ranking: 0.5264\n",
            "  Smoothness: 129.2216\n",
            "  Sparsity: 148.1023\n",
            "  Saved checkpoint: checkpoints/epoch_50.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 51: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.5978, rank=0.5692]\n",
            "Epoch 51/100\n",
            "  Loss: 0.5699\n",
            "  Ranking: 0.5482\n",
            "  Smoothness: 127.1744\n",
            "  Sparsity: 143.8705\n",
            "Epoch 52: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.4805, rank=0.4553]\n",
            "Epoch 52/100\n",
            "  Loss: 0.5554\n",
            "  Ranking: 0.5316\n",
            "  Smoothness: 138.2718\n",
            "  Sparsity: 159.2413\n",
            "Epoch 53: 100% 26/26 [01:02<00:00,  2.40s/it, loss=0.5521, rank=0.5293]\n",
            "Epoch 53/100\n",
            "  Loss: 0.5526\n",
            "  Ranking: 0.5300\n",
            "  Smoothness: 133.6638\n",
            "  Sparsity: 148.9966\n",
            "Epoch 54: 100% 26/26 [01:03<00:00,  2.44s/it, loss=0.5834, rank=0.5658]\n",
            "Epoch 54/100\n",
            "  Loss: 0.5427\n",
            "  Ranking: 0.5198\n",
            "  Smoothness: 135.4729\n",
            "  Sparsity: 151.2145\n",
            "  Saved checkpoint: checkpoints/epoch_54.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 55: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.5046, rank=0.4793]\n",
            "Epoch 55/100\n",
            "  Loss: 0.5510\n",
            "  Ranking: 0.5284\n",
            "  Smoothness: 131.5343\n",
            "  Sparsity: 151.8344\n",
            "Epoch 56: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.4192, rank=0.3926]\n",
            "Epoch 56/100\n",
            "  Loss: 0.5293\n",
            "  Ranking: 0.5074\n",
            "  Smoothness: 127.2962\n",
            "  Sparsity: 146.2089\n",
            "  Saved checkpoint: checkpoints/epoch_56.pth\n",
            "  New best model: checkpoints/best_model.pth\n",
            "Epoch 57: 100% 26/26 [01:01<00:00,  2.38s/it, loss=0.4728, rank=0.4474]\n",
            "Epoch 57/100\n",
            "  Loss: 0.5393\n",
            "  Ranking: 0.5152\n",
            "  Smoothness: 141.4539\n",
            "  Sparsity: 160.0566\n",
            "Epoch 58: 100% 26/26 [01:03<00:00,  2.45s/it, loss=0.7001, rank=0.6772]\n",
            "Epoch 58/100\n",
            "  Loss: 0.5394\n",
            "  Ranking: 0.5163\n",
            "  Smoothness: 134.1561\n",
            "  Sparsity: 154.3296\n",
            "Epoch 59: 100% 26/26 [01:01<00:00,  2.36s/it, loss=0.6116, rank=0.5927]\n",
            "Epoch 59/100\n",
            "  Loss: 0.5347\n",
            "  Ranking: 0.5127\n",
            "  Smoothness: 128.6331\n",
            "  Sparsity: 145.7790\n",
            "Epoch 60: 100% 26/26 [01:01<00:00,  2.37s/it, loss=0.5679, rank=0.5434]\n",
            "Epoch 60/100\n",
            "  Loss: 0.5398\n",
            "  Ranking: 0.5171\n",
            "  Smoothness: 130.6235\n",
            "  Sparsity: 152.4059\n",
            "  Saved checkpoint: checkpoints/epoch_60.pth\n",
            "Epoch 61: 100% 26/26 [01:02<00:00,  2.39s/it, loss=0.7065, rank=0.6788]\n",
            "Epoch 61/100\n",
            "  Loss: 0.5463\n",
            "  Ranking: 0.5230\n",
            "  Smoothness: 135.4711\n",
            "  Sparsity: 155.1438\n",
            "Epoch 62:  77% 20/26 [00:48<00:10,  1.80s/it, loss=0.5343, rank=0.5221]"
          ]
        }
      ],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python train.py --config configs/default.yaml --no-wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA6dRmILhxXZ"
      },
      "source": [
        "## 11. Evaluate\n",
        "\n",
        "Evaluate trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ9N5V8khxXZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "!PYTHONPATH=/content/MILRankingLoss_Sultani2018_ReImplementation:$PYTHONPATH python evaluate.py \\\n",
        "    --config configs/default.yaml \\\n",
        "    --checkpoint checkpoints/best_model.pth \\\n",
        "    --temporal-annotation data/annotations/Temporal_Anomaly_Annotation_for_Testing_Videos.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nwS2O59hxXZ"
      },
      "source": [
        "## 12. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OlxpzFLhxXZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Display ROC curve\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "if os.path.exists('results/roc_curve.png'):\n",
        "    display(Image('results/roc_curve.png'))\n",
        "\n",
        "# Print evaluation results\n",
        "if os.path.exists('results/evaluation_summary.txt'):\n",
        "    !cat results/evaluation_summary.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQmURsohxXZ"
      },
      "source": [
        "## 13. (Optional) Save Results to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8V9kmCvhxXa"
      },
      "outputs": [],
      "source": [
        "%cd /content/MILRankingLoss_Sultani2018_ReImplementation\n",
        "\n",
        "# Backup checkpoints and results to Drive\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data_distribution'\n",
        "!mkdir -p \"{DRIVE_DATA_PATH}/results\"\n",
        "!cp -r checkpoints \"{DRIVE_DATA_PATH}/\"\n",
        "!cp -r results \"{DRIVE_DATA_PATH}/\"\n",
        "\n",
        "print(\"Results saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My new model:"
      ],
      "metadata": {
        "id": "G_0aR1z_ikpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ImprovedAnomalyDetector(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved 3-layer fully connected network for anomaly detection with dropout, batch normalization, and LeakyReLU.\n",
        "    Architecture:\n",
        "    - FC1: 4096 -> 512 (LeakyReLU + BatchNorm + Dropout 0.5)\n",
        "    - FC2: 512 -> 64 (LeakyReLU + BatchNorm + Dropout 0.5)\n",
        "    - FC3: 64 -> 1 (Sigmoid)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4096, dropout=0.5):\n",
        "        super(ImprovedAnomalyDetector, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, num_segments, feature_dim)\n",
        "                e.g., (30, 32, 4096)\n",
        "\n",
        "        Returns:\n",
        "            scores: (batch_size, num_segments)\n",
        "                    Anomaly score for each segment (0~1)\n",
        "        \"\"\"\n",
        "        batch_size, num_segments, feature_dim = x.shape\n",
        "\n",
        "        # Reshape to process all segments at once\n",
        "        x = x.view(-1, feature_dim)  # (batch_size * num_segments, 4096)\n",
        "\n",
        "        # FC layers with BatchNorm, LeakyReLU, and Dropout\n",
        "        x = self.fc1(x)           # (batch_size * num_segments, 512)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)           # (batch_size * num_segments, 64)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)           # (batch_size * num_segments, 1)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        # Reshape back\n",
        "        scores = x.view(batch_size, num_segments)  # (batch_size, 32)\n",
        "\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "lVDAetGgij-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImprovedMILRankingLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple Instance Learning Ranking Loss with sparsity and smoothness constraints.\n",
        "\n",
        "    Loss formula from paper:\n",
        "    loss = hinge_loss + λ1 * smoothness + λ2 * sparsity\n",
        "\n",
        "    where:\n",
        "    - hinge_loss = max(0, 1 - max(pos_scores) + max(neg_scores))\n",
        "    - smoothness = sum of squared differences between adjacent segments\n",
        "    - sparsity = sum of all positive bag scores\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambda1=0.00008, lambda2=0.00008, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lambda1: weight for temporal smoothness constraint\n",
        "            lambda2: weight for sparsity constraint\n",
        "            alpha: factor to control how much sparsity and smoothness contribute to the final loss\n",
        "        \"\"\"\n",
        "        super(ImprovedMILRankingLoss, self).__init__()\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "        self.alpha = alpha  # Regularization factor for sparsity and smoothness terms\n",
        "\n",
        "    def forward(self, pos_scores, neg_scores):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pos_scores: (batch_pos, num_segments) - scores for positive bags\n",
        "            neg_scores: (batch_neg, num_segments) - scores for negative bags\n",
        "\n",
        "        Returns:\n",
        "            loss: scalar tensor\n",
        "        \"\"\"\n",
        "        # MIL ranking loss: max score of positive bag should be higher than negative\n",
        "        pos_max = torch.max(pos_scores, dim=1)[0]  # (batch_pos,)\n",
        "        neg_max = torch.max(neg_scores, dim=1)[0]  # (batch_neg,)\n",
        "\n",
        "        # Hinge loss\n",
        "        ranking_loss = torch.clamp(\n",
        "            1.0 - pos_max.mean() + neg_max.mean(),\n",
        "            min=0\n",
        "        )\n",
        "\n",
        "        # Temporal smoothness: minimize difference between adjacent segments\n",
        "        smoothness_loss = 0\n",
        "        if pos_scores.size(1) > 1:  # if more than 1 segment\n",
        "            temporal_diff = pos_scores[:, 1:] - pos_scores[:, :-1]  # (batch, 31)\n",
        "            smoothness_loss = torch.sum(temporal_diff ** 2)\n",
        "\n",
        "        # Sparsity: minimize sum of all scores (encourage sparse anomalies)\n",
        "        sparsity_loss = torch.sum(pos_scores)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = ranking_loss + self.lambda1 * smoothness_loss + self.lambda2 * sparsity_loss\n",
        "\n",
        "        # Optionally scale smoothness and sparsity with alpha to control how much they contribute\n",
        "        total_loss += self.alpha * (self.lambda1 * smoothness_loss + self.lambda2 * sparsity_loss)\n",
        "\n",
        "        return total_loss, {\n",
        "            'ranking_loss': ranking_loss.item(),\n",
        "            'smoothness_loss': smoothness_loss.item() if isinstance(smoothness_loss, torch.Tensor) else 0.0,\n",
        "            'sparsity_loss': sparsity_loss.item()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "BVZFIF82iqcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# Assuming all necessary classes like ImprovedAnomalyDetector, MILRankingLoss, and C3DFeatureDataset are defined above this block\n",
        "\n",
        "def build_model(config, device):\n",
        "    \"\"\"Build model and move to device\"\"\"\n",
        "    model = ImprovedAnomalyDetector(  # Use the improved model\n",
        "        input_dim=config['model']['input_dim'],\n",
        "        dropout=config['model']['dropout']\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_optimizer(model, config):\n",
        "    \"\"\"Build optimizer\"\"\"\n",
        "    optimizer_name = config['training']['optimizer'].lower()\n",
        "    lr = config['training']['learning_rate']\n",
        "    weight_decay = config['training']['lambda3']\n",
        "\n",
        "    if optimizer_name == 'adamw':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_ranking_loss = 0.0\n",
        "    epoch_smoothness_loss = 0.0\n",
        "    epoch_sparsity_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        pos_features = batch['pos_features']\n",
        "        neg_features = batch['neg_features']\n",
        "\n",
        "        if pos_features is None or neg_features is None:\n",
        "            continue\n",
        "\n",
        "        pos_features = pos_features.to(device)\n",
        "        neg_features = neg_features.to(device)\n",
        "\n",
        "        # Forward\n",
        "        pos_scores = model(pos_features)\n",
        "        neg_scores = model(neg_features)\n",
        "\n",
        "        # Loss\n",
        "        loss, loss_dict = criterion(pos_scores, neg_scores)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_ranking_loss += loss_dict['ranking_loss']\n",
        "        epoch_smoothness_loss += loss_dict['smoothness_loss']\n",
        "        epoch_sparsity_loss += loss_dict['sparsity_loss']\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss.item():.4f}\",\n",
        "            'rank': f\"{loss_dict['ranking_loss']:.4f}\"\n",
        "        })\n",
        "\n",
        "    num_batches = len(loader)\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    avg_ranking = epoch_ranking_loss / num_batches\n",
        "    avg_smoothness = epoch_smoothness_loss / num_batches\n",
        "    avg_sparsity = epoch_sparsity_loss / num_batches\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'ranking_loss': avg_ranking,\n",
        "        'smoothness_loss': avg_smoothness,\n",
        "        'sparsity_loss': avg_sparsity\n",
        "    }\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, save_path):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = Path('checkpoints/New Model')\n",
        "    checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Build dataset\n",
        "    train_dataset = C3DFeatureDataset(\n",
        "        annotation_path=config['data']['train_annotation_path'],\n",
        "        features_root=config['data']['feature_path']\n",
        "    )\n",
        "    print(f\"Train dataset: {len(train_dataset)} videos\")\n",
        "\n",
        "    # Build sampler and loader\n",
        "    sampler = BalancedBatchSampler(\n",
        "        train_dataset,\n",
        "        batch_size=config['training']['batch_size']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_sampler=sampler,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=4\n",
        "    )\n",
        "    print(f\"Total batches per epoch: {len(train_loader)}\")\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(config, device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Build optimizer\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    print(f\"Optimizer: {config['training']['optimizer']}\")\n",
        "\n",
        "    # Build loss\n",
        "    criterion = MILRankingLoss(\n",
        "        lambda1=config['training']['lambda1'],\n",
        "        lambda2=config['training']['lambda2']\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = config['training']['num_epochs']\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        metrics = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"  Loss: {metrics['loss']:.4f}\")\n",
        "        print(f\"  Ranking: {metrics['ranking_loss']:.4f}\")\n",
        "        print(f\"  Smoothness: {metrics['smoothness_loss']:.4f}\")\n",
        "        print(f\"  Sparsity: {metrics['sparsity_loss']:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch % 10 == 0 or metrics['loss'] < best_loss:\n",
        "            save_path = checkpoint_dir / f'epoch_{epoch}.pth'\n",
        "            save_checkpoint(model, optimizer, epoch, metrics['loss'], save_path)\n",
        "            print(f\"  Saved checkpoint: {save_path}\")\n",
        "\n",
        "            if metrics['loss'] < best_loss:\n",
        "                best_loss = metrics['loss']\n",
        "                best_path = checkpoint_dir / 'best_model.pth'\n",
        "                save_checkpoint(model, optimizer, epoch, metrics['loss'], best_path)\n",
        "                print(f\"  New best model: {best_path}\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Dummy config for testing, this should be passed as a parameter or defined earlier\n",
        "    config = {\n",
        "        'model': {'input_dim': 4096, 'dropout': 0.5},\n",
        "        'training': {'optimizer': 'adamw', 'learning_rate': 1e-4, 'lambda3': 0.01, 'num_epochs': 20, 'batch_size': 32, 'lambda1': 0.00008, 'lambda2': 0.00008},\n",
        "        'data': {'train_annotation_path': 'path_to_annotations', 'feature_path': 'path_to_features'}\n",
        "    }\n",
        "    main(config)\n"
      ],
      "metadata": {
        "id": "4-GlZrA6lC_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# Assuming ImprovedAnomalyDetector and C3DFeatureDataset are already defined in the notebook\n",
        "\n",
        "def load_temporal_annotations(annotation_file):\n",
        "    \"\"\"Load temporal annotations for test videos.\"\"\"\n",
        "    annotations = {}\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 6:\n",
        "                continue\n",
        "            video_name = parts[0].replace('.mp4', '')  # Remove extension\n",
        "            start1, end1 = int(parts[2]), int(parts[3])\n",
        "            start2, end2 = int(parts[4]), int(parts[5])\n",
        "            segments = []\n",
        "            if start1 != -1 and end1 != -1:\n",
        "                segments.append((start1, end1))\n",
        "            if start2 != -1 and end2 != -1:\n",
        "                segments.append((start2, end2))\n",
        "            annotations[video_name] = segments\n",
        "    return annotations\n",
        "\n",
        "\n",
        "def get_frame_level_labels(video_name, annotations, num_segments=32, fps=30):\n",
        "    \"\"\"Generate binary labels for video segments (0 = normal, 1 = anomaly).\"\"\"\n",
        "    labels = np.zeros(num_segments, dtype=np.int32)\n",
        "    base_name = video_name.split('/')[-1]\n",
        "    if base_name not in annotations:\n",
        "        return labels\n",
        "    anomaly_segments = annotations[base_name]\n",
        "    if not anomaly_segments:\n",
        "        return labels\n",
        "    max_frame = max(end for _, end in anomaly_segments)\n",
        "    frames_per_segment = max_frame / num_segments\n",
        "    for seg_idx in range(num_segments):\n",
        "        seg_start = seg_idx * frames_per_segment\n",
        "        seg_end = (seg_idx + 1) * frames_per_segment\n",
        "        for anomaly_start, anomaly_end in anomaly_segments:\n",
        "            if not (seg_end < anomaly_start or seg_start > anomaly_end):\n",
        "                labels[seg_idx] = 1\n",
        "                break\n",
        "    return labels\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataset, annotations, device):\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n",
        "            sample = dataset[idx]\n",
        "            features = sample['features'].unsqueeze(0).to(device)  # (1, 32, 4096)\n",
        "            video_name = sample['video_name']\n",
        "\n",
        "            # Get predictions\n",
        "            scores = model(features).squeeze(0).cpu().numpy()  # (32,)\n",
        "\n",
        "            # Get ground truth labels\n",
        "            labels = get_frame_level_labels(video_name, annotations)\n",
        "\n",
        "            all_labels.extend(labels)\n",
        "            all_scores.extend(scores)\n",
        "\n",
        "    return np.array(all_labels), np.array(all_scores)\n",
        "\n",
        "\n",
        "def plot_roc_curve(labels, scores, save_path):\n",
        "    \"\"\"Plot and save ROC curve\"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"ROC curve saved to {save_path}\")\n",
        "\n",
        "    return roc_auc, fpr, tpr, thresholds\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def plot_pr_curve(labels, scores, save_path):\n",
        "    precision, recall, _ = precision_recall_curve(labels, scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"PR curve saved to {save_path}\")\n",
        "\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "def save_results(labels, scores, save_path):\n",
        "    \"\"\"Save evaluation results\"\"\"\n",
        "    results = {\n",
        "        'labels': labels.tolist(),\n",
        "        'scores': scores.tolist()\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    print(f\"Results saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Directly define the paths (replace with your actual paths)\n",
        "new_model_checkpoint = \"checkpoints/New Model/best_model.pth\"  # Path to your best model\n",
        "temporal_annotation_path = \"data/annotations/test_set.txt\"  # Path to your test annotation file\n",
        "\n",
        "# Define the config dictionary directly (no YAML file)\n",
        "config = {\n",
        "    'model': {\n",
        "        'input_dim': 4096,\n",
        "        'dropout': 0.5\n",
        "    },\n",
        "    'training': {\n",
        "        'optimizer': 'adamw',\n",
        "        'learning_rate': 1e-4,\n",
        "        'lambda3': 0.01,\n",
        "        'num_epochs': 20,\n",
        "        'batch_size': 32,\n",
        "        'lambda1': 0.00008,\n",
        "        'lambda2': 0.00008\n",
        "    },\n",
        "    'data': {\n",
        "        'test_annotation_path': \"data/annotations/test_set.txt\",  # Using the test annotation path\n",
        "        'feature_path': \"data/features\"  # Replace with actual feature path\n",
        "    }\n",
        "}\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load new model\n",
        "new_model = ImprovedAnomalyDetector(\n",
        "    input_dim=config['model']['input_dim'],\n",
        "    dropout=config['model']['dropout']\n",
        ")\n",
        "\n",
        "# Load new model checkpoint\n",
        "new_model_checkpoint = torch.load(new_model_checkpoint, map_location=device)\n",
        "new_model.load_state_dict(new_model_checkpoint['model_state_dict'])\n",
        "new_model = new_model.to(device)\n",
        "\n",
        "print(f\"Loaded new model from {new_model_checkpoint}\")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = C3DFeatureDataset(\n",
        "    annotation_path=config['data']['test_annotation_path'],\n",
        "    features_root=config['data']['feature_path']\n",
        ")\n",
        "\n",
        "print(f\"Test dataset: {len(test_dataset)} videos\")\n",
        "\n",
        "# Load temporal annotations\n",
        "annotations = load_temporal_annotations(temporal_annotation_path)\n",
        "print(f\"Loaded temporal annotations for {len(annotations)} videos\")\n",
        "\n",
        "# Evaluate new model\n",
        "print(\"\\nEvaluating new model...\")\n",
        "new_labels, new_scores = evaluate_model(new_model, test_dataset, annotations, device)\n",
        "\n",
        "# Calculate ROC curve\n",
        "roc_auc, fpr, tpr, thresholds = plot_roc_curve(new_labels, new_scores, 'results/new_model_roc_curve.png')\n",
        "\n",
        "# Save results for new model\n",
        "save_results(new_labels, new_scores, 'results/new_model_evaluation_results.json')\n",
        "\n",
        "# Summary of evaluation results\n",
        "print(f\"\\nTotal segments evaluated: {len(new_labels)}\")\n",
        "print(f\"Anomaly segments: {new_labels.sum()} ({new_labels.sum()/len(new_labels)*100:.1f}%)\")\n",
        "print(f\"Normal segments: {len(new_labels) - new_labels.sum()} ({(len(new_labels)-new_labels.sum())/len(new_labels)*100:.1f}%)\")\n",
        "\n",
        "# Find optimal threshold (Youden's J statistic)\n",
        "j_scores = tpr - fpr\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(f\"\\nOptimal threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"  TPR: {tpr[optimal_idx]:.4f}\")\n",
        "print(f\"  FPR: {fpr[optimal_idx]:.4f}\")\n",
        "\n",
        "# Save summary\n",
        "results_dir = Path('results/New Model')\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "summary_path = results_dir / 'evaluation_summary.txt'\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(f\"Evaluation Summary\\n\")\n",
        "    f.write(f\"{'='*60}\\n\")\n",
        "    f.write(f\"Model: {new_model_checkpoint}\\n\")\n",
        "    f.write(f\"Test videos: {len(test_dataset)}\\n\")\n",
        "    f.write(f\"Total segments: {len(new_labels)}\\n\")\n",
        "    f.write(f\"Anomaly segments: {new_labels.sum()} ({new_labels.sum()/len(new_labels)*100:.1f}%)\\n\")\n",
        "    f.write(f\"\\nResults:\\n\")\n",
        "    f.write(f\"  AUC: {roc_auc:.4f}\\n\")\n",
        "    f.write(f\"  Optimal threshold: {optimal_threshold:.4f}\\n\")\n",
        "    f.write(f\"  TPR at optimal: {tpr[optimal_idx]:.4f}\\n\")\n",
        "    f.write(f\"  FPR at optimal: {fpr[optimal_idx]:.4f}\\n\")\n",
        "\n",
        "print(f\"\\nSummary saved to {summary_path}\")\n"
      ],
      "metadata": {
        "id": "PZMrGRyOl0kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define the file paths for both models\n",
        "baseline_roc_path = 'results/baseline_roc_curve.png'  # Path for the baseline model ROC curve\n",
        "new_model_roc_path = 'results/new_model_roc_curve.png'  # Path for the new model ROC curve\n",
        "\n",
        "# Display ROC curve for baseline model\n",
        "if os.path.exists(baseline_roc_path):\n",
        "    print(\"Displaying ROC curve for Baseline Model:\")\n",
        "    display(Image(baseline_roc_path))\n",
        "\n",
        "# Display ROC curve for new model\n",
        "if os.path.exists(new_model_roc_path):\n",
        "    print(\"Displaying ROC curve for New Model:\")\n",
        "    display(Image(new_model_roc_path))\n",
        "\n",
        "# Define the file paths for evaluation results of both models\n",
        "baseline_eval_results_path = 'results/baseline_evaluation_results.json'  # Path for baseline model results\n",
        "new_model_eval_results_path = 'results/new_model_evaluation_results.json'  # Path for new model results\n",
        "\n",
        "# Display evaluation results for baseline model\n",
        "if os.path.exists(baseline_eval_results_path):\n",
        "    print(\"\\nEvaluation results for Baseline Model:\")\n",
        "    with open(baseline_eval_results_path, 'r') as f:\n",
        "        baseline_results = json.load(f)\n",
        "    print(f\"Labels: {baseline_results['labels'][:5]}...\")  # Print first 5 labels\n",
        "    print(f\"Scores: {baseline_results['scores'][:5]}...\")  # Print first 5 scores\n",
        "\n",
        "# Display evaluation results for new model\n",
        "if os.path.exists(new_model_eval_results_path):\n",
        "    print(\"\\nEvaluation results for New Model:\")\n",
        "    with open(new_model_eval_results_path, 'r') as f:\n",
        "        new_model_results = json.load(f)\n",
        "    print(f\"Labels: {new_model_results['labels'][:5]}...\")  # Print first 5 labels\n",
        "    print(f\"Scores: {new_model_results['scores'][:5]}...\")  # Print first 5 scores\n"
      ],
      "metadata": {
        "id": "JUI7vGnPnLCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}